{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MA6202: Laboratorio de Ciencia de Datos\n",
    "\n",
    "#### Profesor: Nicolás Caro\n",
    "\n",
    "#### Fecha de entrega: 17/05/2020\n",
    "\n",
    "#### Integrantes: Matías Romero, Danner Schlotterbeck, Kurt Walsen\n",
    "\n",
    "# Tarea 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:35.612316Z",
     "start_time": "2020-05-20T03:15:33.620226Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P1 Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:35.764353Z",
     "start_time": "2020-05-20T03:15:35.615309Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se carga una de las semanas de la data para inspeccionar:\n",
    "w13_all = pd.read_csv('./data/raw/w{}/metrocuadrado_all_w{}.csv'.format(13,13))\n",
    "w13_fur= pd.read_csv('./data/raw/w{}/metrocuadrado_furnished_w{}.csv'.format(13,13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:35.781308Z",
     "start_time": "2020-05-20T03:15:35.767344Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inspeccionamos sus columnas, y notamos que son las mismas\n",
    "print(w13_all.columns)\n",
    "print(w13_fur.columns)\n",
    "w13_all.columns==w13_fur.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:35.901990Z",
     "start_time": "2020-05-20T03:15:35.784300Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Realizamos una concatenación con y sin duplicados para verificar la \n",
    "existencia de éstos.\n",
    "'''\n",
    "df_con=pd.concat([w13_all,w13_fur],ignore_index=True)\n",
    "df_con.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:36.049628Z",
     "start_time": "2020-05-20T03:15:35.904983Z"
    }
   },
   "outputs": [],
   "source": [
    "# Verificamos si existen duplicados\n",
    "df_con.shape[0]==df_con.drop_duplicates().shape[0] # False, luego existen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:36.237980Z",
     "start_time": "2020-05-20T03:15:36.052588Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Buscamos generar una variable categórica que indique si la observación\n",
    "correspondiente proviene de un archivo que en su nombre contiene `furnished`.\n",
    "Para ello haremos uso de la función merge.\n",
    "\n",
    "Notamos que la columna que sirve de identificador es `url` pues debiese \n",
    "ser único para cada propiedad en arriendo. Además podemos utilizarlo como llave\n",
    "para hacer el merge de los dataframes y ver qué ocurre con al columna\n",
    "`furnished`.\n",
    "\n",
    "De esta manera identificamos a archivos de texto `furnished` que no estén\n",
    "contenidos en archivos con texto `all` mediante la inspección de los valores\n",
    "right_only en la columna 'furnished'.\n",
    "'''\n",
    "w13_all = w13_all.drop_duplicates()\n",
    "w13_fur = w13_fur.drop_duplicates()\n",
    "\n",
    "\n",
    "df_mer = pd.merge(left=w13_all,right=w13_fur,on='url',how='outer',indicator='furnished')\n",
    "df_mer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:36.352191Z",
     "start_time": "2020-05-20T03:15:36.241968Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Para w13 podemos notar 5 de éstos casos. Pero hay que verlo para el caso\n",
    "general.\n",
    "'''\n",
    "fur=df_mer.drop_duplicates()[['url','furnished']]\n",
    "fur[fur['furnished']=='right_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:36.609500Z",
     "start_time": "2020-05-20T03:15:36.356179Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Ahora, agregamos al dataframe de concatenación la columna con la variable\n",
    "categórica furnished, mediante el uso de merge nuevamente.\n",
    "'''\n",
    "df_con = df_con.drop_duplicates()\n",
    "pd.merge(df_con,fur,on='url').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:36.651389Z",
     "start_time": "2020-05-20T03:15:36.613491Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se compara con el DataFrame sin la columna agregada, se observa que poseen\n",
    "la misma cantidad de registros.\n",
    "'''\n",
    "df_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:36.806973Z",
     "start_time": "2020-05-20T03:15:36.654380Z"
    }
   },
   "outputs": [],
   "source": [
    "w13_fur.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:36.939631Z",
     "start_time": "2020-05-20T03:15:36.809965Z"
    }
   },
   "outputs": [],
   "source": [
    "w13_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:37.171037Z",
     "start_time": "2020-05-20T03:15:36.943608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Para unificar formatos, pasamos las columnas 'n_rooms' y 'n_bath' de _fur a string.\n",
    "w13_fur.n_rooms=w13_fur.n_rooms.astype(str)\n",
    "w13_fur.n_bath=w13_fur.n_bath.astype(str)\n",
    "w13_fur.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:38.221545Z",
     "start_time": "2020-05-20T03:15:37.174029Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Ahora, realizamos el proceso general para todas las semanas disponibles\n",
    "en la data.\n",
    "'''\n",
    "# Se concatenan todos los _all\n",
    "df_all = pd.concat([pd.read_csv('./data/raw/w{}/metrocuadrado_all_w{}.csv'.format(i,i)) for i in range(13,18)])\n",
    "print('Número de registros en df_all: '+str(len(df_all)))\n",
    "df_all.drop_duplicates(inplace=True)\n",
    "print('Número de registros en df_all sin duplicados: '+str(len(df_all)))\n",
    "\n",
    "# Se concatenan todos los _fur\n",
    "df_fur = pd.concat([pd.read_csv('./data/raw/w{}/metrocuadrado_furnished_w{}.csv'.format(i,i)) for i in range(13,18)])\n",
    "print('Número de registros en df_fur: '+str(len(df_fur)))\n",
    "df_fur.drop_duplicates(inplace=True)\n",
    "df_fur.reset_index()\n",
    "print('Número de registros en df_fur sin duplicados: '+str(len(df_fur)))\n",
    "\n",
    "furnished_only = df_fur.query(\"url not in @df_all.url\")\n",
    "print(\"Observaciones de archivos con texto 'furnished' que no estén\\n contenidos en archivos con texto 'all': \",\n",
    "      len(furnished_only))\n",
    "# Se agregan los que estan solo en furnished\n",
    "df = pd.concat([df_all, df_fur.query(\"url not in @df_all.url\")],ignore_index=True)\n",
    "\n",
    "# Se crea columna dummy y se le asigna valor 1 a los que se encontraban en archivos furnished\n",
    "from_furnished = df.query(\"url in @df_fur.url\").index\n",
    "df.loc[:, 'furnished'] = np.zeros(len(df), dtype=int)\n",
    "df.loc[from_furnished, 'furnished'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:38.251466Z",
     "start_time": "2020-05-20T03:15:38.224538Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:38.571610Z",
     "start_time": "2020-05-20T03:15:38.254456Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se procede a limpiar parte de la data, esto incluye cambiar el formato\n",
    "de ciertas variables de interés. En particular:\n",
    "\n",
    "- 'price' : Se decidió expresar la columna de precios como un número flotante,\n",
    "            para evitar posibles futuras complicaciones en la inferencia \n",
    "            estadística.\n",
    "            \n",
    "- 'n_rooms' : Se decidió expresar la columna de número de habitaciones como \n",
    "              str, pues debido a la forma de los valores en esta columna\n",
    "              es posible categorizarlas en base a la cantidad de baños \n",
    "              que registra, de esta manera los datos del tipo '5+' serán\n",
    "              parte de una única categoría.\n",
    "- 'n_bath' : Análogo a 'n_rooms'.\n",
    "              \n",
    "'''\n",
    "# Se actualiza la columna 'price'\n",
    "df['price'] = df['price'].str.replace('.','').str.strip('$').map(float)\n",
    "\n",
    "# Se genera un diccionario de reemplazos\n",
    "repl_dic = {1.0:'1',2.0:'2',3.0:'3',4.0:'4',5.0:'5'}\n",
    "\n",
    "# Se actualizan las columnas 'n_rooms' y 'n_bath'\n",
    "df['n_rooms'] = df['n_rooms'].replace(repl_dic)\n",
    "df['n_bath'] = df['n_bath'].replace(repl_dic)\n",
    "\n",
    "\n",
    "'''Se procede a separar el contenido de la columna \n",
    "'property_type|rent_type|location' en tres nuevas columnas 'property_type', \n",
    "'rent_type' y 'location'.\n",
    "'''\n",
    "# Preparamos la columna a tratar\n",
    "newcols = df['property_type|rent_type|location'].str.split(pat= ',',expand=True)\n",
    "\n",
    "# Se generan las nuevas columnas\n",
    "df['location'] = newcols[1]\n",
    "df[['property_type','rent_type']]=newcols[0].str.lower().str.split(pat=' ',n=1,expand=True)\n",
    "df.loc[:,'rent_type'] = df['rent_type'].str.lstrip('en ')\n",
    "\n",
    "\n",
    "# Fijamos las columnas de df en orden\n",
    "cols = ['property_type', 'rent_type', 'location', 'price', 'n_rooms',\n",
    "        'n_bath', 'surface', 'details', 'url', 'metrocuadrado_index',\n",
    "        'furnished']\n",
    "# Actualizamos df\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:38.635438Z",
     "start_time": "2020-05-20T03:15:38.574601Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se procede a agregar las columnas 'price_per_m2' y 'n_garajes',donde:\n",
    "\n",
    "- 'price_per_m2' : Representa el precio por metro cuadrado.\n",
    "\n",
    "- 'n_garajes' : Representa el número de garajes.\n",
    "\n",
    "'''\n",
    "# Modificamos los valores de la columna 'surface' para que representen el\n",
    "# valor en metros cuadrados como número flotante.\n",
    "\n",
    "df['surface'] = df['surface'].replace('m2', '', regex=True).map(float)\n",
    "\n",
    "# Generamos la nueva columna 'price_per_m2' \n",
    "df['price_per_m2'] = df['price'] / df['surface']\n",
    "\n",
    "\n",
    "\n",
    "'''Para 'n_garajes' fue un proceso más complejo. Notamos que el str \n",
    "asociado a cada elemento en 'url' es de la siguiente forma:\n",
    "'''\n",
    "print(df['url'][0])\n",
    "\n",
    "'''Sin embargo, este no es el caso de todas las url. En particular:\n",
    "'''\n",
    "print(df['url'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:38.779630Z",
     "start_time": "2020-05-20T03:15:38.637433Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Luego, una manera intuitiva de obtener el número de garajes es separar el\n",
    "str con el separador '-garajes' lo cual generará una lista de dos strings, \n",
    "luego podemos acceder al primer string de la lista y extraer el último elemento,\n",
    "recuperando así el número de garajes.\n",
    "Lo cual nos entrega los siguientes valores para 'n_garajes':\n",
    "'''\n",
    "df['n_garajes'] = df['url'].map(lambda string: string.split(sep='-garajes')[0][-1])\n",
    "\n",
    "df['n_garajes'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:38.956596Z",
     "start_time": "2020-05-20T03:15:38.782622Z"
    }
   },
   "outputs": [],
   "source": [
    "#ojo acá\n",
    "print(df['url'][3])\n",
    "df['n_garajes'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:39.139108Z",
     "start_time": "2020-05-20T03:15:38.962580Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Notamos que aparece un elemento de la forma '+', luego al igual que\n",
    "'n_rooms' y 'n_bath', existe una categoria que representa una cantidad\n",
    "superior a cierto valor, la intuición nos dice que tal categoria\n",
    "corresponde a '9+'.\n",
    "En búsqueda de generalizar la transformación(y así incluir éste tipo de\n",
    "categorías) se uso una lambda function distinta, dando como resultado \n",
    "lo siguiente:\n",
    "'''\n",
    "df['n_garajes'] = df['url'].map(lambda string: string.split(sep='-garajes')[0][-1] if string.split(sep='-garajes')[0][-1] !='+' else string.split(sep='-garajes')[0][-2:])\n",
    "df['n_garajes'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:39.278734Z",
     "start_time": "2020-05-20T03:15:39.143097Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Ésto nos muestra que la única categoria de la forma antes descrita \n",
    "corresponde solamente a '4+', algo confuso pues ya existen las categorías 5:9.\n",
    "Notamos que la cantidad de datos que corresponden a éste tipo de categoría \n",
    "son 9. \n",
    "'''\n",
    "print(len(df['n_garajes'][df['n_garajes']=='4+']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:39.483053Z",
     "start_time": "2020-05-20T03:15:39.281727Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Veamos ahora cúantos corresponden a las categorías >4, notamos \n",
    "que corresponden a 1010(bastante mayor en comparación a '4+').\n",
    "'''\n",
    "print(len(df['n_garajes'][df['n_garajes']>'4']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:39.706967Z",
     "start_time": "2020-05-20T03:15:39.486046Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Sin embargo, notamos que la cantidad de datos que pueden representarse\n",
    "en la categoría '4+' corresponden a 1010(notar que al computar '4+'>'4'\n",
    "nos entrega True), una cifra bastante menor a los 16299 datos totales de\n",
    "los cuales disponemos, más precisamente, tenemos que las variables que superan\n",
    "la categoría '4' corresponden en promedio a 1010/6=168.3 por categoría, mientras \n",
    "que las variables en categoría '4' o inferior corresponden en promedio a \n",
    "(16299-1010)/5=3057.8 por categoría. En base a ésto, es correcto aseverar que\n",
    "podemos agrupar las variables con categoria superior a '4' en una única \n",
    "categoría '4+', sin perder variabilidad en el feature 'n_garajes'.\n",
    "Por lo tanto, generamos un diccionario que mapee todos las categorias\n",
    "resultantes superiores a '4' en una única categoría '4+'.\n",
    "'''\n",
    "# Se genera diccionario de mapeos\n",
    "map_dict={'+':'4+', '5':'4+', '6':'4+', '7':'4+', '8':'4+', '9':'4+'}\n",
    "\n",
    "# Se genera la correcta columna 'n_garajes'\n",
    "df['n_garajes'] = df['url'].map(lambda string: string[\n",
    "    string.find('-garajes')-1] if string.find('-garajes') > 0 else np.nan )\n",
    "df['n_garajes'] = df['n_garajes'].replace(map_dict)\n",
    "df['n_garajes'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:39.858589Z",
     "start_time": "2020-05-20T03:15:39.712952Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fijamos el orden de las columnas en df.\n",
    "cols = ['property_type', 'rent_type', 'location', 'price','price_per_m2', \n",
    "        'n_rooms', 'n_bath', 'n_garajes','surface', 'details', 'url', \n",
    "        'metrocuadrado_index', 'furnished']\n",
    "\n",
    "# Reordenamos df.\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:40.006168Z",
     "start_time": "2020-05-20T03:15:39.863549Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:40.213612Z",
     "start_time": "2020-05-20T03:15:40.009160Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Queremos categorizar los productos disponibles en la data según\n",
    "el tipo de inmueble al que corresponde y la cantidad de m2 de \n",
    "superficie que poseen, para ello haremos uso de 8 categorias:\n",
    "\n",
    "1 : 'rent_type' = 'casa' , 80 <= 'surface' < 120\n",
    "2 : 'rent_type' = 'casa' , 120 <= 'surface' < 180\n",
    "3 : 'rent_type' = 'casa' , 180 <= 'surface' < 240\n",
    "4 : 'rent_type' = 'casa' , 240 <= 'surface' < 360\n",
    "5 : 'rent_type' = 'casa' , 360 <= 'surface' < 460\n",
    "6 : 'rent_type' = 'apartamento' , 40 <= 'surface' < 60\n",
    "7 : 'rent_type' = 'apartamento' , 60 <= 'surface' < 80\n",
    "8 : 'rent_type' = 'apartamento' , 80 <= 'surface' < 120\n",
    "\n",
    "'''\n",
    "\n",
    "# Se crea columna para ser rellenada a posteriori\n",
    "df['product_type'] = np.repeat([np.nan], len(df))\n",
    "\n",
    "# Se rellenan los tipos para las casas\n",
    "cotas_casas = [(80, 120), (120, 180), (180, 240), (240, 360), \n",
    "               (360,460)]\n",
    "\n",
    "for i in range(len(cotas_casas)):\n",
    "    q = \"(property_type == 'casa') & ({0} < surface <= {1})\".format(*cotas_casas[i])\n",
    "    idx = df.query(q).index\n",
    "    df.loc[idx, 'product_type'] = str(i+1)\n",
    "\n",
    "# Se rellenan los tipos para apartamentos    \n",
    "cotas_apartamentos = [(40, 60), (60, 80), (80, 120)]\n",
    "\n",
    "for i in range(len(cotas_apartamentos)):\n",
    "    q = \"(property_type == 'apartamento') & ({0} < surface <= {1})\".format(*cotas_apartamentos[i])\n",
    "    idx = df.query(q).index\n",
    "    df.loc[idx, 'product_type'] = str(i+(len(cotas_casas)+1))\n",
    "    \n",
    "print('Cantidad de productos no clasificados: '\n",
    "      ,len(df[df['product_type'].isna()]))\n",
    "print('Categorías: ',df['product_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:40.311351Z",
     "start_time": "2020-05-20T03:15:40.215606Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T01:15:15.139288Z",
     "start_time": "2020-05-14T01:15:15.109933Z"
    }
   },
   "source": [
    "## Parte 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:40.461947Z",
     "start_time": "2020-05-20T03:15:40.314345Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Queremos generar una nueva columna que indique el barrio, a partir de location\n",
    "Notamos que todas las location poseen la estructuca '{barrio} Bogotá D.C.'\n",
    "'''\n",
    "df.location.unique()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:40.596311Z",
     "start_time": "2020-05-20T03:15:40.466934Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Verificamos si hay locations que no contengan esta keyword, \n",
    "notamos que todos la tienen.\n",
    "'''\n",
    "df[df['location'].map(lambda string: not('Bogotá D.C.' in string))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:40.791819Z",
     "start_time": "2020-05-20T03:15:40.599302Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Procedemos a generar la columna 'barrio', de manera similar a como \n",
    "obtuvimos la columna 'n_garajes'\n",
    "'''\n",
    "df.loc[df.index, 'barrio'] = df['location'].map(lambda string: \n",
    "                                                string.split(sep='Bogotá')[0].strip(' ').lower())\n",
    "print('Cantidad única de barrios disponibles: ',len(df['barrio'].unique()))\n",
    "df[['location','barrio']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:40.993797Z",
     "start_time": "2020-05-20T03:15:40.794780Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se examina barrio-upz.csv para determinar cómo hacer el merge\n",
    "upz=pd.read_csv('./data/asignacion_upz/barrio-upz.csv')\n",
    "upz.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:41.205774Z",
     "start_time": "2020-05-20T03:15:40.996759Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Cantidad única de UPlNombre disponibles: ',len(upz.UPlNombre.unique()))\n",
    "upz.UPlNombre.unique()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:41.330269Z",
     "start_time": "2020-05-20T03:15:41.208762Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Cantidad única de pro_location disponibles: ',len(upz.pro_location.unique()))\n",
    "upz.pro_location.unique()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:41.690817Z",
     "start_time": "2020-05-20T03:15:41.333261Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Notamos que 'pro_location' presenta una gama más amplia de barrios sobre los\n",
    "cuales poder cruzar la data. En base a esto, haremos un cruce entre df y\n",
    "asignacion_upz.\n",
    "'''\n",
    "# Se realiza un merge 'outer' para determinar cuántos no tienen código upz\n",
    "df_merged = pd.merge(df,upz,left_on='barrio',right_on='pro_location',\n",
    "                     how='outer',indicator='ind')\n",
    "\n",
    "barrios_upz = df_merged[df_merged['ind']=='right_only']['UPlNombre'].nunique()\n",
    "obs_sin = len(df_merged[df_merged['ind']=='left_only'])\n",
    "print('Barrios con código UPZ que no están en df: '+str(barrios_upz))\n",
    "print('Observaciones sin código UPZ: '+str(obs_sin))\n",
    "print('Barrios sin código UPZ: '+str(df_merged[df_merged['ind']=='left_only'].barrio.nunique()))\n",
    "porc = int(100*len(df_merged[df_merged['ind']=='both'])/len(df))\n",
    "print('El {}% de las observaciones tienen código UPZ'.format(porc))\n",
    "\n",
    "'''Notamos que existen 13 barrios de la data upz los cuales no\n",
    "aparecen en nuestro df, ésto se quizás a que no existen publicaciones de \n",
    "arriendo/venta de propiedades pertenecientes a tales barrios.\n",
    "Además, se observa que 1946 registros pertenecen a barrios los cuales no se \n",
    "les puede adjuntar un código UPZ. De éstos notamos que la cantidad única de \n",
    "barrios corresponde a 176. Además, un 88% de las observaciones en df poseen\n",
    "un código UPZ.\n",
    "'''\n",
    "\n",
    "# Se define el nuevo df\n",
    "#df = df_merged[df_merged['ind']=='both'].drop(columns=['UPlTipo','UPlNombre','ind','pro_location'])\n",
    "df = df_merged[df_merged['ind']!='right_only'].drop(columns=['UPlTipo','UPlNombre','ind','pro_location'])\n",
    "\n",
    "df.reset_index(drop=True)\n",
    "\n",
    "# Fijamos las columnas en df.\n",
    "cols = ['product_type','property_type', 'rent_type', 'location','barrio','UPlCodigo',\n",
    "        'UPlArea', 'price','price_per_m2','surface', 'n_rooms', 'n_bath',\n",
    "        'n_garajes', 'details', 'url', 'metrocuadrado_index', 'furnished']\n",
    "\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:41.752658Z",
     "start_time": "2020-05-20T03:15:41.694805Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:41.905258Z",
     "start_time": "2020-05-20T03:15:41.755651Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cargamos la data asociada a estadisticas_poblacion.csv\n",
    "stats_pob=pd.read_csv('./data/estadisticas_upz/estadisticas_poblacion.csv')\n",
    "stats_pob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:42.043400Z",
     "start_time": "2020-05-20T03:15:41.908251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Estructura columna de UPlCodigo para la data stats_pob, a priori\n",
    "# no se observas complicaciones en el formato.\n",
    "stats_pob.upz.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:42.297720Z",
     "start_time": "2020-05-20T03:15:42.046392Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rescatamos solo las columnas relevantes\n",
    "stats_pob.drop(columns=['Unnamed: 0','nomupz'],inplace=True)\n",
    "df_merged = pd.merge(df,stats_pob,left_on='UPlCodigo',right_on='upz',how='left')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:42.351576Z",
     "start_time": "2020-05-20T03:15:42.299716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cargamos la data asociada a indice_inseguridad.csv\n",
    "ind_inseg=pd.read_csv('./data/estadisticas_upz/indice_inseguridad.csv')\n",
    "ind_inseg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:42.462855Z",
     "start_time": "2020-05-20T03:15:42.354568Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notamos que hay códigos '1', '3', '4' y '5', pero también notamos\n",
    "# que éstos UPZ{} no están en df ni coinciden con otros, por lo que \n",
    "# los ignoramos.\n",
    "print('Códigos UPZ en ind_inseg: ',ind_inseg.UPlCodigo.unique())\n",
    "ind_inseg.drop(columns=['Unnamed: 0','UPlNombre2'],inplace=True)\n",
    "df_merged = pd.merge(df_merged,ind_inseg,on='UPlCodigo',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:42.637388Z",
     "start_time": "2020-05-20T03:15:42.464849Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:42.696747Z",
     "start_time": "2020-05-20T03:15:42.640382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cargamos la data asociada a porcentaje_areas_verdes.csv\n",
    "perc_areas_verdes = pd.read_csv('./data/estadisticas_upz/porcentaje_areas_verdes.csv')\n",
    "perc_areas_verdes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:42.871280Z",
     "start_time": "2020-05-20T03:15:42.698741Z"
    }
   },
   "outputs": [],
   "source": [
    "# Estructura columna de UPlCodigo para la data perc_areas_verdes,\n",
    "# notamos que contiene solo floats, sin el prefijo 'UPZ'\n",
    "print('Previo a la transformación:\\n',perc_areas_verdes['cod_upz'].unique())\n",
    "# Se tranforma x en str de la forma 'UPZ'+str(int(x))\n",
    "perc_areas_verdes.loc[:,'cod_upz']=perc_areas_verdes['cod_upz'].map(lambda x: 'UPZ'+str(int(x)))\n",
    "print('Luego de la transformación:\\n',perc_areas_verdes['cod_upz'].unique())\n",
    "\n",
    "# Se dropean columnas innecesarias y se hace un último merge\n",
    "perc_areas_verdes.drop(columns=['Unnamed: 0','upz'],inplace=True)\n",
    "df_merged = pd.merge(df_merged,perc_areas_verdes,left_on='UPlCodigo',right_on='cod_upz',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:43.126596Z",
     "start_time": "2020-05-20T03:15:42.874271Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:43.183444Z",
     "start_time": "2020-05-20T03:15:43.132581Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hay 3 columnas que dicen lo mismo: 'UPlCodigo','upz','cod_upz'. Nos quedamos con la segunda por simplicidad\n",
    "df=df_merged.drop(columns=['UPlCodigo','cod_upz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:43.319083Z",
     "start_time": "2020-05-20T03:15:43.186437Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generamos la nueva columna con la densidad de población por UPZ\n",
    "df['densidad_poblacion']=df['personas']/df['UPlArea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:43.634240Z",
     "start_time": "2020-05-20T03:15:43.322074Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Botamos los últimos duplicados\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:43.642218Z",
     "start_time": "2020-05-20T03:15:43.637231Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se guarda una copia de la data procesada hasta ahora\n",
    "#df_raw = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:43.762895Z",
     "start_time": "2020-05-20T03:15:43.646207Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creamos una función que fije cómo queremos que se vean los plots.\n",
    "def estilo():\n",
    "    sns.set(style='darkgrid')\n",
    "    plt.rcParams['figure.figsize'] = (18, 18)\n",
    "estilo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:43.921480Z",
     "start_time": "2020-05-20T03:15:43.764889Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Buscamos hacer un perfilamiento de las variables disponibles en la data\n",
    "a partir de la parte anterior.\n",
    "'''\n",
    "print(df.shape)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:44.041157Z",
     "start_time": "2020-05-20T03:15:43.924468Z"
    }
   },
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:44.166551Z",
     "start_time": "2020-05-20T03:15:44.045146Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tipos de variables que queremos\n",
    "names = ['numeric','categorical','miscelaneous']\n",
    "\n",
    "# Fijamos las variables numéricas\n",
    "numeric = ['UPlArea','price','surface','metrocuadrado_index',\n",
    "           'personas', 'trabajoinf_ninos_5_17_anos_perc',\n",
    "           'trabajoinfampliado_ninos_5_17_anos_perc',\n",
    "           'jovenes_14_24_anos_nini_perc','indice_envegecimiento',\n",
    "           'jefe_mujer_perc','adultos_mayores_pobres_perc','indice_inseguridad',\n",
    "           'areas_verdes_perc','densidad_poblacion','price_per_m2']\n",
    "\n",
    "# Fijamos las variables miscelaneas, recordemos que el barrio\n",
    "# puede identificarse con un código upz\n",
    "miscelaneous= ['location','barrio','url','details']\n",
    "\n",
    "# Se crea una lista con las variables categoricas\n",
    "categorical = list((set(df.columns) - set(numeric)) - set(miscelaneous))\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:44.314669Z",
     "start_time": "2020-05-20T03:15:44.169542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generamos el mapeo a multi-índices\n",
    "mapping = [('numeric', col) for col in numeric]\n",
    "mapping.extend([('categorical', col) for col in categorical])\n",
    "mapping.extend([('miscelaneous', col) for col in miscelaneous])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:44.577965Z",
     "start_time": "2020-05-20T03:15:44.316664Z"
    }
   },
   "outputs": [],
   "source": [
    "# Re-indexamos la data\n",
    "df = df.reindex(columns=numeric + categorical + miscelaneous)\n",
    "df.columns = pd.MultiIndex.from_tuples(mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:44.705623Z",
     "start_time": "2020-05-20T03:15:44.580956Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Variables numéricas\n",
    "\n",
    "Previo a graficar las variables numéricas, debemos notar que existen\n",
    "registros en los cuales 'surface' es 0( y por ende 'price_per_m2' es inf)\n",
    "'''\n",
    "zero_surf=df[df[('numeric','surface')] == 0][[('numeric','price'),('numeric','surface'),\n",
    "                                   ('numeric','price_per_m2')]]\n",
    "nonzero_surf=df[df[('numeric','surface')] > 0][[('numeric','price'),('numeric','surface'),\n",
    "                                   ('numeric','price_per_m2')]]\n",
    "print(\"Total de registros: \",len(df))\n",
    "print(\"Registros con 'surface'=0 : \",len(zero_surf))\n",
    "print(\"Registros con 'surface'>0 : \",len(nonzero_surf))\n",
    "zero_surf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:44.863205Z",
     "start_time": "2020-05-20T03:15:44.708614Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Luego podemos considerar estos datos como faltantes, pues en la\n",
    "práctica no existen propiedades de 0 m2 con tales precios.\n",
    "'''\n",
    "df.loc[:,[('numeric','surface')]] = df[[('numeric','surface')]].replace(float(0),np.nan)\n",
    "df.loc[:,[('numeric','price_per_m2')]] = df[[('numeric','price_per_m2')]].replace(np.inf,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:44.972908Z",
     "start_time": "2020-05-20T03:15:44.866194Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verificamos que el cambio se realizó correctamente\n",
    "df[df[('numeric','surface')].isnull()][[('numeric','price'),('numeric','surface'),\n",
    "                                   ('numeric','price_per_m2')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:45.297550Z",
     "start_time": "2020-05-20T03:15:44.975900Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Guardaremos una copia de df para poder hacer modificaciones en\n",
    "el camino y de esta manera entender mejor las visualizaciones. \n",
    "Más adelante veremos que tales modificaciones resultan útiles a la hora\n",
    "de poder obtener una mejor identificación entre las variables.\n",
    "'''\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:58.480601Z",
     "start_time": "2020-05-20T03:15:45.300546Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se procede a graficar las distribuciones de las variables numéricas\n",
    "'''\n",
    "# Grilla de subplots\n",
    "fig, ax = plt.subplots(nrows=4, ncols=4)#, figsize=[17, 17])\n",
    "\n",
    "# Se remueven el ultimo plot\n",
    "list(map(lambda a : a.remove(), ax[-1,-1:]))\n",
    "\n",
    "# Se ajusta el espaciado exterior de la figura\n",
    "fig.tight_layout()\n",
    "\n",
    "# Se define un titulo y su ubicacion\n",
    "fig.suptitle('Distribuciones Univariadas Numéricas',\n",
    "             fontsize=20,\n",
    "             x=0.5,\n",
    "             y=1.05)\n",
    "'''\n",
    "Se recorre cada axis, para cada columna del dataframe, se genera un grafico \n",
    "distinto en funcion del tipo de dato.\n",
    "\n",
    "'''\n",
    "for axis, col in zip(ax.flatten(), numeric):\n",
    "    try :\n",
    "        # Graficos para datos numericos\n",
    "        sns.distplot(df[('numeric', col)].dropna(), ax=axis, rug=True)\n",
    "               \n",
    "    except RuntimeError:\n",
    "        sns.distplot(df[('numeric', col)].dropna(), ax=axis, rug=True, kde=False)\n",
    "    \n",
    "    axis.set_xlabel(col, fontsize=15)\n",
    "\n",
    "# Se ajusta el espaciado interno entre subplots\n",
    "w, h = (.4, .4)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:58.602276Z",
     "start_time": "2020-05-20T03:15:58.483597Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Notamos en 'price' y 'price per_m2' que hay una diferencia muy brusca en la\n",
    "densidad de los datos, donde la tendencia es clara a precios mas moderados en\n",
    "comparacion.\n",
    "Dado que 'price_per_m2' es nuestra variable respuesta, nos interesa que esta\n",
    "presente una distribución que sea lo suficientemente suave para \n",
    "poder realizar futuras transformaciones en nuestro futuro modelo regresor.\n",
    "Notamos que considerando datos con valor de 'price_per_m2' < 100.000, se \n",
    "obtiene el siguiente dataframe.\n",
    "'''\n",
    "df[df[('numeric','price_per_m2')]<=10**5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:59.476857Z",
     "start_time": "2020-05-20T03:15:58.605269Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Tenemos una diferencia de 100 datos con la data original, es decir,\n",
    "estamos ignorando los 100 valores donde 'price_per_m2' > 100.000. Veamos \n",
    "ahora como se ve su distribución.\n",
    "'''\n",
    "sns.distplot(df[df[('numeric','price_per_m2')] <= 10**5][('numeric','price_per_m2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:15:59.600526Z",
     "start_time": "2020-05-20T03:15:59.481844Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Notamos una clara mejora con respecto al escenario con toda la data.\n",
    "Actualizamos df y vemos ahora como distribuyen todas las variables numericas\n",
    "asociadas.\n",
    "'''\n",
    "df = df[df[('numeric','price_per_m2')] <= 10**5]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:12.417833Z",
     "start_time": "2020-05-20T03:15:59.603517Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se procede a graficar las distribuciones de las variables numéricas\n",
    "luego del ajuste\n",
    "'''\n",
    "# Grilla de subplots\n",
    "fig, ax = plt.subplots(nrows=4, ncols=4)#, figsize=[17, 17])\n",
    "\n",
    "# Se remueven el ultimo plot\n",
    "list(map(lambda a : a.remove(), ax[-1,-1:]))\n",
    "\n",
    "# Se ajusta el espaciado exterior de la figura\n",
    "fig.tight_layout()\n",
    "\n",
    "# Se define un titulo y su ubicacion\n",
    "fig.suptitle('Distribuciones Univariadas Numéricas',\n",
    "             fontsize=20,\n",
    "             x=0.5,\n",
    "             y=1.05)\n",
    "'''\n",
    "Se recorre cada axis, para cada columna del dataframe, se genera un grafico \n",
    "distinto en funcion del tipo de dato.\n",
    "\n",
    "'''\n",
    "for axis, col in zip(ax.flatten(), numeric):\n",
    "    try :\n",
    "        # Graficos para datos numericos\n",
    "        sns.distplot(df[('numeric', col)].dropna(), ax=axis, rug=True)\n",
    "               \n",
    "    except RuntimeError:\n",
    "        sns.distplot(df[('numeric', col)].dropna(), ax=axis, rug=True, kde=False)\n",
    "    \n",
    "    axis.set_xlabel(col, fontsize=15)\n",
    "\n",
    "# Se ajusta el espaciado interno entre subplots\n",
    "w, h = (.4, .4)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:12.431765Z",
     "start_time": "2020-05-20T03:16:12.419825Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Notamos como la distribución de la data es más clara al trabajar sin los casos\n",
    "donde price_per_m2 es demasiado grande. Notamos en surface que existe un dato que se aleja\n",
    "mucho de donde se concentra la data, y su valor es al menos superior a 1000, donde los \n",
    "demas valores se concentran por debajo de 1000. Buscamos tal valor\n",
    "'''\n",
    "df[('numeric','surface')].nlargest(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:13.311243Z",
     "start_time": "2020-05-20T03:16:12.435753Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Veamos qué ocurre al quitar tal valor del dataframe, notamos una\n",
    "clara mejora en la visualización.\n",
    "'''\n",
    "sns.distplot(df[df[('numeric','surface')]< 10**3][('numeric','surface')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:13.337163Z",
     "start_time": "2020-05-20T03:16:13.317216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se actualiza df\n",
    "df = df[df[('numeric','surface')]< 10**3]\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:23.814863Z",
     "start_time": "2020-05-20T03:16:13.340156Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grilla de subplots\n",
    "fig, ax = plt.subplots(nrows=4, ncols=4)#, figsize=[17, 17])\n",
    "\n",
    "# Se remueven el ultimo plot\n",
    "list(map(lambda a : a.remove(), ax[-1,-1:]))\n",
    "\n",
    "# Se ajusta el espaciado exterior de la figura\n",
    "fig.tight_layout()\n",
    "\n",
    "# Se define un titulo y su ubicacion\n",
    "fig.suptitle('Distribuciones Univariadas Numéricas',\n",
    "             fontsize=20,\n",
    "             x=0.5,\n",
    "             y=1.05)\n",
    "'''\n",
    "Se recorre cada axis, para cada columna del dataframe, se genera un grafico \n",
    "distinto en funcion del tipo de dato.\n",
    "\n",
    "'''\n",
    "for axis, col in zip(ax.flatten(), numeric):\n",
    "    try :\n",
    "        # Graficos para datos numericos\n",
    "        sns.distplot(df[('numeric', col)].dropna(), ax=axis, rug=True)\n",
    "               \n",
    "    except RuntimeError:\n",
    "        sns.distplot(df[('numeric', col)].dropna(), ax=axis, rug=True, kde=False)\n",
    "    \n",
    "    axis.set_xlabel(col, fontsize=15)\n",
    "\n",
    "# Se ajusta el espaciado interno entre subplots\n",
    "w, h = (.4, .4)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:23.836804Z",
     "start_time": "2020-05-20T03:16:23.818852Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Queremos ver como se comporta la variable 'price_per_m2' en respuesta\n",
    "a algunas variables numéricas.\n",
    "'''\n",
    "def scatter_dists(col, df=df, h=.3, w=.1, fontdict={'fontsize': 20}, reg=True):\n",
    "    ''' Recibe una columna numerica y genera una visualizacion comparativa.\n",
    "    \n",
    "    Genera una figura por sobre el dataframe (por defecto), recibe \n",
    "    parametros extra como el espaciado entre subfigura.\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "    \n",
    "    col: String\n",
    "         El nombre de la columna numerica a visualizar\n",
    "    \n",
    "    h,w: float\n",
    "        Espaciado entre subplot h -> vertical, w -> horizontal\n",
    "    \n",
    "    fontdict: dict\n",
    "             Permite configurar las fuentes de los subplots\n",
    "    reg: bool\n",
    "         Permite graficar una regresion lineal sobre los datos (if True)\n",
    "        \n",
    "    Returns: None\n",
    "        Se muestra una figura en pantalla    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Estrucutra de figura y axes\n",
    "    fig, ax = plt.subplots(2, 1, figsize=[12, 13])\n",
    "\n",
    "    # violin plot --> equivalente a catplot(kind = 'violin')\n",
    "\n",
    "    if reg:\n",
    "        sns.regplot(x=df[('numeric', col)],\n",
    "                    y=df[('numeric', 'price_per_m2')],\n",
    "                    ax=ax[0])\n",
    "        ax[0].set_title('Regplot plot {} vs price_per_m2'.format(col), fontdict)\n",
    "    else:\n",
    "        sns.scatterplot(('numeric', col),\n",
    "                        y=('numeric', 'price_per_m2'),\n",
    "                        data=df,\n",
    "                        ax=ax[0])\n",
    "        ax[0].set_title('Scatter plot {} vs price_per_m2'.format(col), fontdict)\n",
    "\n",
    "    \n",
    "    # Distribucion univariada\n",
    "    sns.distplot(df[('numeric', col)].dropna(), ax=ax[1])\n",
    "\n",
    "    ax[0].set_xlabel(col, fontdict)\n",
    "    ax[1].set_xlabel(col, fontdict)\n",
    "\n",
    "    ax[0].set_ylabel('price_per_m2', fontdict)\n",
    "    ax[1].set_title('Frecuencias {}'.format(col), fontdict)\n",
    "\n",
    "    plt.subplots_adjust(wspace=w, hspace=h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:26.054459Z",
     "start_time": "2020-05-20T03:16:23.853758Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Notamos que la variable 'metrocuadrado_index', presenta una buena \n",
    "distribución en la data y presenta un comportamiento lineal con ruido, puede que esta \n",
    "variable sea de interés a la hora de regresionar 'price_per_m2'.\n",
    "'''\n",
    "scatter_dists('metrocuadrado_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:28.078012Z",
     "start_time": "2020-05-20T03:16:26.060412Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Notamos que las variables 'price' y 'surface' presentan un comportamiento\n",
    "lineal con respecto a 'price_per_m2', pero esto se debe a como se generó la\n",
    "columna 'price_per_m2', luego la relación existente entre éstas variables\n",
    "fue impuesta y no presenta un caso de interés.\n",
    "'''\n",
    "scatter_dists('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:30.125101Z",
     "start_time": "2020-05-20T03:16:28.082998Z"
    }
   },
   "outputs": [],
   "source": [
    "scatter_dists('surface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:32.015849Z",
     "start_time": "2020-05-20T03:16:30.129088Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Ruido, 'price_per_m2' no presenta ninguna respuesta clara ante\n",
    "esta variable. Su significancia deberá ser evaluada más adelante.\n",
    "\n",
    "Resultado análogo entre\n",
    "'UPlArea'\n",
    "'trabajoinf_ninos_5_17_anos_perc'\n",
    "'trabajoinfampliado_ninos_5_17_anos_perc'\n",
    "'jovenes_14_24_anos_nini_perc'\n",
    "'indice_envegecimiento'\n",
    "'jefe_mujer_perc'\n",
    "'adultos_mayores_pobres_perc'\n",
    "'indice_inseguridad'\n",
    "'areas_verdes_perc'\n",
    "'densidad_poblacion'\n",
    "'''\n",
    "scatter_dists('UPlArea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:32.029813Z",
     "start_time": "2020-05-20T03:16:32.018853Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Variables categóricas\n",
    "Procedemos a analizar las variables categóricas\n",
    "'''\n",
    "print(len(categorical))\n",
    "print(categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:36.009744Z",
     "start_time": "2020-05-20T03:16:32.032804Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se procede a graficar los histogramas de las variables categóricas\n",
    "'''\n",
    "# Grilla de subplots\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3)\n",
    "\n",
    "# Se remueven el ultimo plot\n",
    "list(map(lambda a : a.remove(), ax[-1,-1:]))\n",
    "\n",
    "# Se ajusta el espaciado exterior de la figura\n",
    "fig.tight_layout()\n",
    "# Se define un titulo y su ubicacion\n",
    "fig.suptitle('Distribuciones Univariadas Categóricas',\n",
    "             fontsize=20,\n",
    "             x=0.5,\n",
    "             y=1.05)\n",
    "'''\n",
    "Se recorre cada axis, para cada columna del dataframe, se genera un grafico \n",
    "distinto en funcion del tipo de dato.\n",
    "\n",
    "'''\n",
    "for axis, col in zip(ax.flatten(), categorical):\n",
    "    # Graficos para datos tipos str\n",
    "    sns.countplot(df[('categorical',col)], ax=axis)\n",
    "    axis.set_axis_off()\n",
    "    axis.set_title(col, fontsize=15)\n",
    "  \n",
    "    \n",
    "# Se ajusta el espaciado interno entre subplots\n",
    "h, w = (.4, .1)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:36.026699Z",
     "start_time": "2020-05-20T03:16:36.012705Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Estudiemos la variabilidad de las variables categóricas ''\n",
    "'''\n",
    "# Función para generar gráficos\n",
    "def categoricalplot(df,col,log=False):\n",
    "    # Sirve para fija el tamaño de las etiquetas del plot\n",
    "    fontdict = {'fontsize':20}\n",
    "\n",
    "    # Estrucutra de figura y axes\n",
    "    fig, ax = plt.subplots(2,1,figsize=[12,13])\n",
    "    \n",
    "    # violin plot --> equivalente a catplot(kind = 'violin')\n",
    "\n",
    "    if log:\n",
    "        sns.violinplot(('categorical', col),\n",
    "                    y=('numeric', 'price_per_m2'),\n",
    "                    data=df,\n",
    "                    kind='violin',\n",
    "                    ax=ax[0]).set_yscale('log')\n",
    "    \n",
    "    else:\n",
    "        sns.violinplot(('categorical', col),\n",
    "                    y=('numeric', 'price_per_m2'),\n",
    "                    data=df,\n",
    "                    kind='violin',\n",
    "                    ax=ax[0])\n",
    "    \n",
    "    sns.countplot(df[('categorical',col)], ax=ax[1])\n",
    "\n",
    "    ax[0].set_xlabel(col, fontdict)\n",
    "    ax[1].set_xlabel(col, fontdict)\n",
    "\n",
    "    ax[0].set_ylabel('precio_per_m2', fontdict)\n",
    "    ax[0].set_title('Violin plot {} vs price_per_m2'.format(col), fontdict)\n",
    "    ax[1].set_title('Frecuencias {}'.format(col), fontdict)\n",
    "\n",
    "    h, w = (.3, .1)\n",
    "    plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:49.150781Z",
     "start_time": "2020-05-20T03:16:36.029658Z"
    }
   },
   "outputs": [],
   "source": [
    "'''upz\n",
    "Observamos que la variable categoriza upz presenta un histograma a la vista\n",
    "confuso(demasiadas categorias), por lo que más adelante buscaremos una mejor\n",
    "manera de agrupar éstas categorías.\n",
    "'''\n",
    "categoricalplot(df,'upz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:50.117419Z",
     "start_time": "2020-05-20T03:16:49.158805Z"
    }
   },
   "outputs": [],
   "source": [
    "'''product_type\n",
    "Notamos que para los 'product_type' 1-5 (casas), presentan una\n",
    "concentración en valores levemente más bajos que los 6-8 (apartamentos), además\n",
    "la variabilidad del valor de precio dado si es casa o apartamento presentan una\n",
    "distribución similar. Por ende product_type podría corresponder a una variable\n",
    "de interés a la hora de definir 'price_per_m2'.\n",
    "'''\n",
    "categoricalplot(df,'product_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:50.750328Z",
     "start_time": "2020-05-20T03:16:50.123387Z"
    }
   },
   "outputs": [],
   "source": [
    "'''property_type\n",
    "Notamos que los valores de 'precio_per_m2' para la categoría 'casa' se concentran\n",
    "en su mayoría en un valor menor que la categoría apartamento. Ésto se condice\n",
    "con el gráfico anterior y se debe en parte a cómo definimos la variable\n",
    "'product_type', luego puede existir cierta correlación entre éstas dos variables,\n",
    "lo cual puede corroborarse mediante un test estadístico.\n",
    "'''\n",
    "categoricalplot(df,'property_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:51.408890Z",
     "start_time": "2020-05-20T03:16:50.755680Z"
    }
   },
   "outputs": [],
   "source": [
    "'''rent_type\n",
    "Notamos que no difieren en mediana y sus distribuciones en 'price_per_m2'\n",
    "se comportan de manera similar, por ende no existe una manera de poder\n",
    "identificar una de las categorías en base al valor de 'price_per_m2'.\n",
    "Por lo tanto, 'rent_type' corresponde a una variable candidata a no ser \n",
    "considerada en el modelo final.\n",
    "'''\n",
    "categoricalplot(df,'rent_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:52.171072Z",
     "start_time": "2020-05-20T03:16:51.412729Z"
    }
   },
   "outputs": [],
   "source": [
    "'''furnished\n",
    "Si bien no se nota una diferencia en los valores donde más se cocentra\n",
    "cada categoría, el hecho de la clara diferencia en la distribución de\n",
    "la variable nos hace dudar sobre la efectividad en describir la variable\n",
    "'price_per_m2', en caso de ser incluída en el modelo. Por lo tanto,\n",
    "más adelante se verá si incluir o no esta variable en el modelo final.\n",
    "'''\n",
    "categoricalplot(df,'furnished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:52.981098Z",
     "start_time": "2020-05-20T03:16:52.174009Z"
    }
   },
   "outputs": [],
   "source": [
    "'''n_rooms\n",
    "Se notan claras diferencias en las medias del valor 'price_per_m2'\n",
    "entre categorias, ademas de una distribución normalmente aproximable\n",
    "en los valores en esta variable. Por lo tanto corresponde a un candidato\n",
    "sólido a ser incluido en el modelo final.\n",
    "'''\n",
    "categoricalplot(df,'n_rooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:53.805593Z",
     "start_time": "2020-05-20T03:16:52.985942Z"
    }
   },
   "outputs": [],
   "source": [
    "'''n_bath\n",
    "Si bien la diferencia de medias en este caso tambien existe pero en menor\n",
    "medida, la forma variabilidad en su distribución puede ser información \n",
    "valiosa para la descripción de 'price_per_m2', por lo tanto a priori es \n",
    "una variable interesante a considerar.\n",
    "'''\n",
    "categoricalplot(df,'n_bath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:54.587799Z",
     "start_time": "2020-05-20T03:16:53.807580Z"
    }
   },
   "outputs": [],
   "source": [
    "'''n_garajes\n",
    "Tenemos un caso similar a 'n_rooms', donde aquí la diferencia de medias\n",
    "es menos clara, y debido a que los violines son más achatados nos hace\n",
    "dudar sobre la correcta descripción de 'price_per_m2' por medio de esta\n",
    "variable. Veremos mediante un test one-way-ANOVA si existe una diferencia\n",
    "significativa entre grupos.\n",
    "'''\n",
    "categoricalplot(df,'n_garajes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:54.599807Z",
     "start_time": "2020-05-20T03:16:54.590169Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se genera una función auxiliar para indexar las columnas en base\n",
    "a su tipo, extraído de la clase 9 del curso.\n",
    "'''\n",
    "def indexer(cols, t_c = df.columns):\n",
    "    '''Genera columnas multinivel a partir de nombres de columna planos.'''\n",
    "    \n",
    "    set_to_tuple = set(*[cols])\n",
    "\n",
    "    tuples = [\n",
    "        i for i in t_c if set_to_tuple.intersection(set(i))\n",
    "    ]\n",
    "    \n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:54.785259Z",
     "start_time": "2020-05-20T03:16:54.602860Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = indexer(['price_per_m2','n_garajes'])\n",
    "grouped = df[idx].groupby(idx[1])\n",
    "total_groups = grouped.groups.keys()\n",
    "groups = [grouped.get_group(i) for i in total_groups]\n",
    "groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:54.895662Z",
     "start_time": "2020-05-20T03:16:54.788556Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se utiliza una función auxiliar para limpiar el formato de cada grupo,\n",
    "extraído de la clase 9 del curso.\n",
    "'''\n",
    "def group_cleaner(group, col, d_f=df):\n",
    "    ''' Limpia un grupo.\n",
    "    Reconoce la categoria del grupo, en la posicion [:,1], \n",
    "    guarda ese nombre y elimina la columna de categoria, \n",
    "    posteriormente renombra la columna.\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "    \n",
    "    group: pandas Groupby object\n",
    "          Recibe una agrupacion para categorias\n",
    "          \n",
    "    Returns:\n",
    "    ----------\n",
    "        pandas Groupby object\n",
    "        Entrega el grupo ordenado.\n",
    "    '''\n",
    "    group_0 = group.copy()\n",
    "    name = group_0.iloc[0,1]\n",
    "    group_0.drop(indexer([col], t_c = d_f.columns), axis=1, inplace=True)\n",
    "    group_0.columns  = ('cat_{}'.format(name),)\n",
    "    \n",
    "    return group_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:55.062610Z",
     "start_time": "2020-05-20T03:16:54.899246Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se procede a limpiar el formato de cada grupo y se realiza el test\n",
    "'''\n",
    "groups_to_test = [group_cleaner(g, 'n_garajes') for g in groups]\n",
    "\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "F,p = f_oneway(*groups_to_test)\n",
    "\n",
    "print('Estadistico F:',F)\n",
    "print('p valor :', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:55.197481Z",
     "start_time": "2020-05-20T03:16:55.066030Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Con esto, rechazamos la nula: 'No hay diferencia significativa entre\n",
    "grupos'. Por lo tanto, 'n_garajes' corresponde a una variable de interés\n",
    "a analizar.\n",
    "'''\n",
    "alpha = 0.05\n",
    "p <= alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:55.365813Z",
     "start_time": "2020-05-20T03:16:55.201502Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Para ver las variables faltantes, recuperamos df_copy previo a las\n",
    "modificaciones hechas en P2.2\n",
    "'''\n",
    "df_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:56.353353Z",
     "start_time": "2020-05-20T03:16:55.369007Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Vemos un esquema general de los valores faltantes en de la data,\n",
    "se puede observar un comportamiento similar en la ausencia de los \n",
    "datos asociados a las estadísticas incluidas mediante un cruce con los\n",
    "códigos upz. Ésto claramente es debido a que al existir barrios donde no \n",
    "fue posible obtener identificación mediante el código upz, no fue posible\n",
    "cruzar las estadísticas en la sección P1.6, por lo tanto la ausencia\n",
    "de las estadísticas se refleja en la ausencia de upz. El tratamiento\n",
    "para estos datos faltantes será eliminarlos, pues debido a no poder\n",
    "recuperar el upz, no podremos recuperar de manera consistente las\n",
    "estadísticas.\n",
    "'''\n",
    "fig, ax = plt.subplots(figsize = [15, 10])\n",
    "msno.matrix(df_copy,ax = ax, sparkline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:57.371638Z",
     "start_time": "2020-05-20T03:16:56.356488Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Esquema ordenado según cantidad de datos faltantes\n",
    "Observamos que las columnas con más datos faltantes correspondes a\n",
    "`product_type` y 'n_garajes'\n",
    "-'product_type' se debe a como la definimos en \n",
    "la sección P1.4, luego depende de parámetros visibles en la data\n",
    "('property_type','rent_type','surface'). Nuestro tratamiento para ésta \n",
    "variable será simplemente eliminarlas.\n",
    "-'n_garajes' se debe a la ausencia de la keyword '-garajes' en url. Por\n",
    "lo tanto, la ausencia de ésta data se puede inferir a partir de la \n",
    "variable 'url', variable sobre la cual fue construida esta columna.\n",
    "\n",
    "Respecto a las columnas 'n_bath','details','n_rooms','price','surface'\n",
    "no es posible determinar un patron claro, más aun cuando la ausencia\n",
    "de las variables en las ultimas 3 mencionadas son pocas (menos de 33).\n",
    "Creemos entonces que ésta información es perdida completamente al azar,\n",
    "pues depende de algo que no estamos viendo reflejado en la data(mal \n",
    "ingreso de los datos, omisión de informacion por parte del vendedor,etc.)\n",
    "Ésta información se intentará imputar asignando media mediante agrupaciónes\n",
    "por 'upz' donde sea posible.\n",
    "'''\n",
    "msno.matrix(df_copy[list(df.isnull().sum().nlargest(19).index)], sparkline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:16:59.352635Z",
     "start_time": "2020-05-20T03:16:57.375068Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Mediante un mapa de calor podemos identificar la correlación\n",
    "entre los valores faltantes, para determinar si existe algún tipo\n",
    "de dependencia en la ausencia de éstos datos.\n",
    "En efecto, notamos como las variables asociadas a las estadísticas\n",
    "incluidas en la sección P1.6 presentan correlación 1 entre ellas y \n",
    "con upz, confirmando entonces la clara dependencia de la ausencia de \n",
    "estos datos en base a la ausencia de upz. \n",
    "Además, notamos una trivial correlación de 1 entre surface y \n",
    "price_per_m2 debido a que la data faltante(surface=0) genera una\n",
    "imposibilidad en el cálculo de price_per_m2(sería infinito).\n",
    "'''\n",
    "fig, ax = plt.subplots(figsize = [15, 10])\n",
    "msno.heatmap(df_copy, ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:00.357372Z",
     "start_time": "2020-05-20T03:16:59.356524Z"
    }
   },
   "outputs": [],
   "source": [
    "'''A través de un dendograma podemos confirmar la relación de datos \n",
    "faltantes, ahora entre grupos. Confirmamos lo mencionado con las \n",
    "estadísticas y upz, junto con la relacion 'surface' y 'price_per_m2'.\n",
    "\n",
    "Se observa cómo 'product_type' no tiene relación alguna en su\n",
    "ausencia de datos con variables como 'property_type','rent_type',\n",
    "'surface', sino mas bien en los rangos de valor de éstas. \n",
    "\n",
    "Además notamos que la ausencia de datos en 'n_garajes' tambien es ajena\n",
    "a la ausencia de datos en otras variables expuestas acá.\n",
    "\n",
    "Concluimos entonces que las variables 'price_per_m2' y 'property_type'\n",
    "presentan un tipo de mecanismo de pérdida de información del tipo MAR,\n",
    "pues su ausencia depende de variables que podemos observar su valor:\n",
    "- 'surface' para 'price_per_m2'\n",
    "- 'property_type','rent_type','surface' para 'product_type'\n",
    "\n",
    "Por otro lado, las estadísticas cumplen la hipótesis MNAR, ya que la ausencia de información\n",
    "en estas variables se explica por la variable ausente 'upz'.\n",
    "'''\n",
    "fig, ax = plt.subplots()\n",
    "msno.dendrogram(df_copy[list(df_copy.isnull().sum().nlargest(19).index)], ax=ax,orientation='top')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:00.397776Z",
     "start_time": "2020-05-20T03:17:00.362516Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Como los nan asociados a upz corresponden a una perdida de informacion del tipo MNAR,\n",
    "nuestro tratamiento será entonces dropear tales filas.\n",
    "Como los nan asociados a product_type corresponden a valores que ensucian la variable\n",
    "respuesta, tambien se procede a dropear tales filas. \n",
    "'''\n",
    "df_copy.dropna(subset=[('categorical', 'upz'),('categorical','product_type')], axis =0,how='any', inplace=True)\n",
    "df_copy.reset_index(drop=True,inplace=True)\n",
    "df_raw = df_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:00.587309Z",
     "start_time": "2020-05-20T03:17:00.399771Z"
    }
   },
   "outputs": [],
   "source": [
    "df_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:00.725880Z",
     "start_time": "2020-05-20T03:17:00.590995Z"
    }
   },
   "outputs": [],
   "source": [
    "# Obtenemos las modas por columna\n",
    "cols= indexer(['n_rooms','n_bath','n_garajes'])\n",
    "modes = df_copy[cols].mode(axis=0,dropna=True)\n",
    "modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:00.911443Z",
     "start_time": "2020-05-20T03:17:00.730682Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generamos un diccionario de mapeos a utilizar en la imputación\n",
    "fill_dict={'n_garajes': modes[('categorical','n_garajes')][0],\n",
    "          'n_rooms': modes[('categorical','n_rooms')][0],\n",
    "          'n_bath': modes[('categorical','n_bath')][0]\n",
    "          }\n",
    "df_copy[cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:01.117465Z",
     "start_time": "2020-05-20T03:17:00.914415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se procede a imputar\n",
    "for col in cols:\n",
    "    df_copy.loc[:,col] = df_copy.fillna(modes[col][0])\n",
    "df_copy[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:01.183641Z",
     "start_time": "2020-05-20T03:17:01.120151Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notamos que todos las modas fueron imputadas\n",
    "df_copy[cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:06.628335Z",
     "start_time": "2020-05-20T03:17:01.186081Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Recordemos como distribuye la variable categorica 'upz'\n",
    "'''\n",
    "sns.countplot(df_copy[('categorical','upz')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:06.759068Z",
     "start_time": "2020-05-20T03:17:06.632011Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Generaremos un dataframe identico al actual pero con los cambios realizados de P2.2\n",
    "para comparar visualizaciones\n",
    "'''\n",
    "df_mod = df_copy.copy()\n",
    "df_mod =  df_mod[(df_mod[('numeric','price_per_m2')]<=10**5) & (df_mod[('numeric','surface')]<=10**3)] \n",
    "df_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:06.915950Z",
     "start_time": "2020-05-20T03:17:06.762162Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Buscamos clusterizar los 'upz' en base a su valor en 'price_per_m2'\n",
    "'''\n",
    "# Se propone clusterizar la variable upz mediante kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clust = 3 # número de clusters a trabajar\n",
    "\n",
    "# Se inicializan en paralelo 2 clusterizaciones\n",
    "clusterizer = KMeans(n_clusters=n_clust)\n",
    "clusterizer_mod = KMeans(n_clusters=n_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:07.117161Z",
     "start_time": "2020-05-20T03:17:06.921166Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clustering mediante la variable de respuesta\n",
    "# Agrupamos datos por upz y tomamos promedio en la variable de respuesta\n",
    "\n",
    "# Generamos agrupaciones\n",
    "grouped_df = df_copy.groupby(by=('categorical','upz')).mean()[('numeric','price_per_m2')]\n",
    "grouped_df_mod = df_mod.groupby(by=('categorical','upz')).mean()[('numeric','price_per_m2')]\n",
    "\n",
    "# Guardamos los labels en una serie indexados por el codigo upz\n",
    "X = clusterizer.fit_predict(grouped_df.to_numpy().reshape(-1,1))\n",
    "X_mod = clusterizer_mod.fit_predict(grouped_df_mod.to_numpy().reshape(-1,1))\n",
    "\n",
    "# Generamos los labels\n",
    "labels = pd.Series(X, index=grouped_df.index, name=('categorical', 'upz_cluster'))\n",
    "labels_mod = pd.Series(X_mod, index=grouped_df_mod.index, name=('categorical', 'upz_cluster'))\n",
    "\n",
    "# Se hace merge de los labels en las datas respectivas\n",
    "df_clust = pd.merge(df_copy, labels, left_on = [('categorical', 'upz')], \n",
    "                    right_on=labels.index ,left_index=True,how='left')\n",
    "df_clust.loc[:,('categorical', 'upz_cluster')] = df_clust[('categorical', 'upz_cluster')].map(lambda x: str(int(x)))\n",
    "df_clust.reset_index(inplace = True,drop = True )\n",
    "\n",
    "df_clust_mod = pd.merge(df_mod, labels_mod, left_on = [('categorical', 'upz')], \n",
    "                    right_on=labels_mod.index ,left_index=True,how='left')\n",
    "df_clust_mod.loc[:,('categorical', 'upz_cluster')] = df_clust_mod[('categorical', 'upz_cluster')].map(lambda x: str(int(x)))\n",
    "df_clust_mod.reset_index(inplace = True,drop = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:07.162028Z",
     "start_time": "2020-05-20T03:17:07.119785Z"
    }
   },
   "outputs": [],
   "source": [
    "df_raw = pd.merge(df_raw, labels_mod, left_on = [('categorical', 'upz')], \n",
    "                    right_on=labels_mod.index ,left_index=True,how='left',validate='one_to_one')\n",
    "df_raw.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:07.285982Z",
     "start_time": "2020-05-20T03:17:07.163030Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vemos los labels de cada data \n",
    "print('Sin filtrar: ',df_clust[('categorical','upz_cluster')].unique())\n",
    "print('Filtrada: ',df_clust_mod[('categorical','upz_cluster')].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:09.291886Z",
     "start_time": "2020-05-20T03:17:07.290073Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Aquí notamos un comportamiento extraño en la distribución de los valores\n",
    "por categoría, nuestra hipótesis es que se debe a la potencial presencia de\n",
    "outliers asociados a la variable 'price_per_m2'. Sin embargo al estar en escala \n",
    "logarítmica podemos observar una potencial diferencia en las medias por grupo,\n",
    "para confirmar ésto, haremos un test oneway ANOVA sobre 'upz_cluster' vs \n",
    "'price_per_m2'.\n",
    "'''\n",
    "categoricalplot(df_clust,'upz_cluster',log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:09.360718Z",
     "start_time": "2020-05-20T03:17:09.293123Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se realiza el test oneway ANOVA para comprobar la diferencia de medias en la data sin filtrar\n",
    "'''\n",
    "idx = indexer(['price_per_m2','upz_cluster'], t_c=df_clust.columns)\n",
    "grouped = df_clust[idx].groupby(idx[1])\n",
    "total_groups = grouped.groups.keys()\n",
    "groups = [group_cleaner(grouped.get_group(str(i)), 'upz_cluster', df_clust) for i in range(n_clust)]\n",
    "\n",
    "F, p = f_oneway(*groups)\n",
    "print('Estadistico F:',F)\n",
    "print('p valor :', p)\n",
    "alfa = 0.05\n",
    "print('p<=alfa: ',p <= alfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:10.187752Z",
     "start_time": "2020-05-20T03:17:09.363710Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Veamos qué ocurre si consideramos las modificaciones realizadas en P2.2 a éste dataframe\n",
    "'''\n",
    "categoricalplot(df_clust_mod,'upz_cluster',log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:10.197555Z",
     "start_time": "2020-05-20T03:17:10.189932Z"
    }
   },
   "outputs": [],
   "source": [
    "''' Notamos que la mejora es considerable en cuanto a la identificación de price_per_m2 mediante\n",
    "esta clusterización, además podemos observar que la cantidad de datos que se dropean con tal de\n",
    "mejorar a ésta magnitud las representaciones e identificaciones solo corresponde a un 0.4% del total.\n",
    "'''\n",
    "diff = df_clust.shape[0]-df_clust_mod.shape[0]\n",
    "perc_diff = 100 *(df_clust.shape[0]-df_clust_mod.shape[0])/df_clust.shape[0]\n",
    "print('Luego del filtro como en P2.2, solo {} datos son dropeados, lo cual corresponde a un {}% de la cantidad total de datos'.format(diff,perc_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:10.434453Z",
     "start_time": "2020-05-20T03:17:10.201496Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se realiza el test oneway ANOVA con la data filtrada como en P2.2 \n",
    "para comprobar la diferencia de medias. \n",
    "Ésto junto con lo expresado en las idenrificaciónes P2.2 creemos que es evidencia suficiente para seguir\n",
    "trabajando con la data filtrada como en P2.2.\n",
    "'''\n",
    "idx = indexer(['price_per_m2','upz_cluster'], t_c=df_clust.columns)\n",
    "grouped_mod = df_clust_mod[idx].groupby(idx[1])\n",
    "total_groups_mod = grouped_mod.groups.keys()\n",
    "groups_mod = [group_cleaner(grouped_mod.get_group(str(i)), 'upz_cluster', df_clust_mod) for i in range(n_clust)]\n",
    "\n",
    "F, p = f_oneway(*groups_mod)\n",
    "print('Estadistico F:',F)\n",
    "print('p valor :', p)\n",
    "alfa = 0.05\n",
    "print('p<=alfa: ',p <= alfa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:10.592915Z",
     "start_time": "2020-05-20T03:17:10.437445Z"
    }
   },
   "outputs": [],
   "source": [
    "# Actualizamos df con el clustering realizado en la parte anterior para la data filtrada\n",
    "df = df_clust_mod.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:10.714212Z",
     "start_time": "2020-05-20T03:17:10.598678Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Veamos ahora si existe dependencia entre variables de interés\n",
    "'''\n",
    "interest = ['price','surface','metrocuadrado_index','personas',\n",
    "            'indice_inseguridad','price_per_m2',\n",
    "            'n_rooms','n_garajes','n_bath']\n",
    "idxs = indexer(interest)\n",
    "idxs.sort()\n",
    "idxs.remove(('numeric', 'price_per_m2'))\n",
    "idxs.append(('numeric', 'price_per_m2'))\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:29.997064Z",
     "start_time": "2020-05-20T03:17:10.718908Z"
    }
   },
   "outputs": [],
   "source": [
    "data = df.reindex(idxs, axis=1).droplevel(0,axis=1).dropna()\n",
    "sns.pairplot(data=data, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:30.065582Z",
     "start_time": "2020-05-20T03:17:30.004182Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Veamos correlacion entre variables\n",
    "'''\n",
    "corrmatrix = df.corr()\n",
    "corrmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:30.180441Z",
     "start_time": "2020-05-20T03:17:30.068436Z"
    }
   },
   "outputs": [],
   "source": [
    "col = indexer(['price_per_m2'])\n",
    "corrmatrix[col].nlargest(20,col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:31.116327Z",
     "start_time": "2020-05-20T03:17:30.183584Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "En primera instancia analizamos la correlación entre todas\n",
    "las variables numéricas mediante un mapa de calor\n",
    "(la varible objetivo están en la última fila).\n",
    "'''\n",
    "corrmat = df['numeric'].corr()\n",
    "columnas = list(corrmat.columns)\n",
    "\n",
    "corrmat = corrmat.reindex(index = columnas, columns = columnas)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[14, 12])\n",
    "\n",
    "sns.heatmap(corrmat, vmin=-.5, vmax=.9, linewidths=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:31.141802Z",
     "start_time": "2020-05-20T03:17:31.119695Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Se puede apreciar alta correlación (positiva y negativa)\n",
    "entre algunos pares de variables. Para ubicarlos hacemos un\n",
    "rearreglo 1D multi-índice y buscamos los que tengan\n",
    "módulo más alto (distinto de 1).\n",
    "'''\n",
    "unoD=corrmat.stack()\n",
    "unoD[unoD[unoD<1].abs().nlargest(20).index][::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:31.311550Z",
     "start_time": "2020-05-20T03:17:31.144819Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se acordó un umbral de 0.7 para definir una correlación\n",
    "potencialmente problemática en cuanto a la colinearidad.\n",
    "\n",
    "En vista de lo anterior notamos una estrecha relación entre\n",
    "\n",
    "- 'jovenes_14_24_anos_nini_perc'\n",
    "- 'adultos_mayores_pobres_perc'\n",
    "- 'indice_envegecimiento'\n",
    "\n",
    "- 'personas'\n",
    "- 'densidad_poblacion'\n",
    "\n",
    "de las cuales elegimos una de cada grupo (la última) para\n",
    "evitar colinearidad.\n",
    "\n",
    "Cabe destacar que el par ('price','surface')\n",
    "también tiene un valor muy alto, pero como estas variables\n",
    "no se considerarán al momento de estimar 'price_per_m2' al\n",
    "ser las generadoras de esta variable, no se estudia con\n",
    "mayor detalle.\n",
    "\n",
    "Por otro lado vemos que 'metrocuadrado_index'\n",
    "tiene una alta correlación con la variable a explicar, por lo\n",
    "que es un buen candidato para la selección final. Para ver cómo\n",
    "se relacionan las otras variables con 'price_per_m2' analizamos\n",
    "su columna.\n",
    "\n",
    "Incluyendo ahora las variables que no presentan una correlación \n",
    "superior al umbral, tenemos a priori los siguientes candidatos\n",
    "a ser incluidos en el modelo final:\n",
    "\n",
    "- 'densidad_poblacion'\n",
    "- 'indice_envegecimiento'\n",
    "- 'jefe_mujer_perc' \n",
    "- 'areas_verdes_perc' \n",
    "- 'trabajoinf_ninos_5_17_anos_perc'\n",
    "- 'trabajoinfampliado_ninos_5_17_anos_perc'\n",
    "- 'indice_inseguridad'\n",
    "\n",
    "'''\n",
    "corrmat['price_per_m2'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:31.577455Z",
     "start_time": "2020-05-20T03:17:31.319336Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Tenemos las siguientes variables categóricas de interés\n",
    "\n",
    "- 'upz_cluster'\n",
    "- 'product_type'\n",
    "- 'n_rooms'\n",
    "- 'n_bath'\n",
    "- 'n_garajes'\n",
    "- 'furnished'\n",
    "\n",
    "Buscamos ahora verificar estadísticamente si las variables generan\n",
    "una diferencia significativa entre los grupos 'price_per_m2'.\n",
    "Para ello haremos test oneway ANOVA.\n",
    "'''\n",
    "\n",
    "'''\n",
    "Notamos que para cada variable considera se rechaza la nula,\n",
    "por lo tanto cada variable presenta una separación estadística\n",
    "considerable para 'price_per_m2'.\n",
    "'''\n",
    "\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Recordemos que 'upz_cluster' y 'n_garajes' ya fueron validadas en P2.2 y P2.4 respectivamente\n",
    "categoric_vars = ['furnished','product_type', 'n_rooms', 'n_bath']\n",
    "\n",
    "for col in categoric_vars:\n",
    "    print('{}:'.format(col))\n",
    "    idx = indexer(['price_per_m2',col])\n",
    "    grouped = df[idx].groupby(idx[1])\n",
    "    total_groups = grouped.groups.keys()\n",
    "    groups = [group_cleaner(grouped.get_group(i), col) for i in total_groups]\n",
    "    \n",
    "    F,p = f_oneway(*groups)\n",
    "    print('Estadistico F:',F)\n",
    "    print('p valor :', p)\n",
    "    alpha = 0.05\n",
    "    reject= p <= alpha\n",
    "    if reject:\n",
    "        print('Se rechaza la nula para {}'.format(col))\n",
    "    else:\n",
    "        print('NO se rechaza la nula para {}'.format(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:31.589249Z",
     "start_time": "2020-05-20T03:17:31.581054Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Nos interesa ahora ver si existe algun tipo de relación entre las variables\n",
    "numéricas y categóricas que estamos considerando.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:31.887301Z",
     "start_time": "2020-05-20T03:17:31.596771Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Procedemos a testear la significancia de cada variable en la\n",
    "descripción de 'price_per_m2' utilizando el test t.\n",
    "'''\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "numeric_vars = ['densidad_poblacion', 'indice_envegecimiento', 'jefe_mujer_perc', \n",
    "               'areas_verdes_perc', 'trabajoinf_ninos_5_17_anos_perc', \n",
    "               'trabajoinfampliado_ninos_5_17_anos_perc', 'indice_inseguridad']\n",
    "\n",
    "ppm2 = np.array(df[('numeric','price_per_m2')])\n",
    "\n",
    "alfa = 0.05\n",
    "n = len(numeric_vars)\n",
    "reject_matrix = np.zeros((n,n))\n",
    "for i in range(n):\n",
    "    var1 = numeric_vars[i]\n",
    "    arr1 = np.array(df[('numeric',var1)].dropna())\n",
    "    print(var1)\n",
    "    for j in range(i+1,n):\n",
    "        var2 = numeric_vars[j]\n",
    "        arr2 = np.array(df[('numeric',var2)].dropna())\n",
    "        #t,p = testt_ind(arr1,arr2,equal_var=False)\n",
    "        t,p = ks_2samp(arr1,arr2)\n",
    "        print('{}:\\n Valor del estadístico: {}\\n Valor de p: {}'.format(var2,t,p))\n",
    "        reject = p < alfa\n",
    "        if reject:\n",
    "            reject_matrix[i][j] = 1\n",
    "            #print('Se rechaza la nula, {} tiene significancia'.format(var))\n",
    "            \n",
    "reject_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:31.911459Z",
     "start_time": "2020-05-20T03:17:31.890924Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "arr1=np.array(df[('numeric','jovenes_14_24_anos_nini_perc')].dropna())\n",
    "arr2=np.array(df[('numeric','adultos_mayores_pobres_perc')].dropna())\n",
    "t,p = ks_2samp(arr1,arr2)\n",
    "print('Valor del estadístico: {}\\n Valor de p: {}'.format(t,p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:32.056282Z",
     "start_time": "2020-05-20T03:17:31.914625Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Generamos un duplicado del df obtenido en la parte anterior\n",
    "para generar ciertas transformaciones.\n",
    "'''\n",
    "#df_transformed = df_clust.copy()\n",
    "df_transformed= df_clust_mod.copy()\n",
    "df_transformed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:32.171650Z",
     "start_time": "2020-05-20T03:17:32.060808Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "columnas = ['UPlArea','price','surface','metrocuadrado_index','personas',\n",
    "           'trabajoinf_ninos_5_17_anos_perc','trabajoinfampliado_ninos_5_17_anos_perc',\n",
    "           'jovenes_14_24_anos_nini_perc','indice_envegecimiento','jefe_mujer_perc',\n",
    "            'adultos_mayores_pobres_perc','indice_inseguridad','areas_verdes_perc',\n",
    "            'areas_verdes_perc','densidad_poblacion','price_per_m2',\n",
    "            \n",
    "            'n_garajes','upz_cluster','furnished','rent_type','n_rooms','product_type',\n",
    "            'property_type','n_bath']\n",
    "\n",
    "cols = indexer(columnas, t_c = df_transformed.columns)\n",
    "df_transformed = df_transformed[cols]\n",
    "df_transformed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:32.338264Z",
     "start_time": "2020-05-20T03:17:32.176903Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Debemos transformar algunas variables categoricas a labels según corresponda\n",
    "'''\n",
    "numerics= indexer(['UPlArea','price','surface','metrocuadrado_index','personas',\n",
    "           'trabajoinf_ninos_5_17_anos_perc','trabajoinfampliado_ninos_5_17_anos_perc',\n",
    "           'jovenes_14_24_anos_nini_perc','indice_envegecimiento','jefe_mujer_perc',\n",
    "           'adultos_mayores_pobres_perc','indice_inseguridad','areas_verdes_perc',\n",
    "           'areas_verdes_perc','densidad_poblacion','price_per_m2'], t_c = df_transformed.columns)\n",
    "ordinales = indexer(['n_rooms','n_bath','n_garajes'], t_c = df_transformed.columns)\n",
    "no_ordinales = indexer(['furnished','rent_type','property_type','product_type','upz_cluster'], t_c = df_transformed.columns)\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:32.747579Z",
     "start_time": "2020-05-20T03:17:32.344604Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = df_transformed[numerics].dropna().copy()\n",
    "idx = data.index\n",
    "#scaler=StandardScaler()\n",
    "#scaler=RobustScaler()\n",
    "scaler=MinMaxScaler()\n",
    "newdata = scaler.fit_transform(data)\n",
    "newdata = pd.DataFrame(newdata, columns=pd.MultiIndex.from_tuples(numerics))\n",
    "df_transformed.loc[:,numerics] = newdata\n",
    "\n",
    "for col in ordinales:\n",
    "    data = df_transformed[col].dropna().copy()\n",
    "    idx = data.index\n",
    "    enc = OrdinalEncoder()\n",
    "    X = data.values.reshape([-1,1])\n",
    "    transformed = enc.fit_transform(X)\n",
    "    newcol = pd.Series(data = transformed.flatten(), index= idx)\n",
    "    df_transformed.loc[idx,col] = newcol\n",
    "    \n",
    "for col in no_ordinales:\n",
    "    data = df_transformed[col].dropna().copy()\n",
    "    idx = data.index\n",
    "    ohenc = OneHotEncoder(categories='auto',sparse=False)\n",
    "    X = data.values.reshape([-1,1])\n",
    "    transformed = ohenc.fit_transform(X)\n",
    "    feature_names = ohenc.get_feature_names()\n",
    "    feature_names = [name.replace('x0_','{}_'.format(col[1])) for name in feature_names]\n",
    "    feature_names = [('categorical',name) for name in feature_names]\n",
    "    newcols = pd.DataFrame(data = transformed, columns=pd.MultiIndex.from_tuples(feature_names), index= idx)\n",
    "    df_transformed = df_transformed.drop(columns=col).join(newcols)\n",
    "\n",
    "df_transformed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:32.763832Z",
     "start_time": "2020-05-20T03:17:32.750572Z"
    }
   },
   "outputs": [],
   "source": [
    "df_transformed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:32.861178Z",
     "start_time": "2020-05-20T03:17:32.765226Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "outlier_detection = DBSCAN(min_samples = 2, eps = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:38.275396Z",
     "start_time": "2020-05-20T03:17:32.864430Z"
    }
   },
   "outputs": [],
   "source": [
    "# se clusteriza y se obtiene la proporción de outliers\n",
    "X = df_transformed.to_numpy()\n",
    "ol = outlier_detection.fit_predict(X)\n",
    "(ol == -1).sum()/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:38.380219Z",
     "start_time": "2020-05-20T03:17:38.278594Z"
    }
   },
   "outputs": [],
   "source": [
    "# asignamos los clusters a una nueva columna y se mapea de forma que outlier=-1, inlier=1\n",
    "df_clust_mod[('categorical', 'outlier')] = ol\n",
    "data = df_clust_mod.copy()\n",
    "data.columns = data.columns.droplevel()\n",
    "data['outlier'] = data['outlier'].map(lambda x: 1 if x >=0 else -1)\n",
    "\n",
    "# se crea una tabla de doble entrada para visualizar las distribuciones\n",
    "kwargs = {'index': data['upz_cluster'], 'columns': data['outlier']}\n",
    "table = pd.crosstab(**kwargs, margins=True)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:38.446929Z",
     "start_time": "2020-05-20T03:17:38.385012Z"
    }
   },
   "outputs": [],
   "source": [
    "'''A primera vista es posible decir que en proporción, los outliers están distribuidos uniformemente \n",
    "a lo largo de los upz_clusters, es decir, no hay alguno de los clusters en particular que tienda a contener \n",
    "más outliers que los demás, sin embargo, aprovechando que las frecuencias observadas son mayores que 5\n",
    "haremos un test chi 2 para asegurar esta inedependencia, si las frecuencias esperadas son también mayores que 5,\n",
    "entonces se puede asegurar la confiabilidad del test\n",
    "'''\n",
    "from scipy.stats import chi2_contingency\n",
    "chi2, p, dof, expected = chi2_contingency(table)\n",
    "print(p < 0.01)\n",
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:38.700987Z",
     "start_time": "2020-05-20T03:17:38.457258Z"
    }
   },
   "outputs": [],
   "source": [
    "# repetimos el proceso esta vez viendo la distribución de outliers con respecto a product_type\n",
    "kwargs['index'] = data['product_type']\n",
    "table = pd.crosstab(**kwargs, margins=True)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:38.724866Z",
     "start_time": "2020-05-20T03:17:38.706450Z"
    }
   },
   "outputs": [],
   "source": [
    "chi2, p, dof, expected = chi2_contingency(table)\n",
    "print(p < 0.01)\n",
    "expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:38.829390Z",
     "start_time": "2020-05-20T03:17:38.729316Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,RegressorMixin\n",
    "\n",
    "class RegresionBayesianaEmpirica(BaseEstimator,RegressorMixin):\n",
    "    \n",
    "    def __init__(self, alpha=0.01, beta=0.01, tol=1e-5, maxiter=200):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        #self.set_params(alpha=alpha_0,beta=beta_0)\n",
    "        self.tol=tol\n",
    "        self.maxiter = maxiter\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "    \n",
    "    @property\n",
    "    def X(self):\n",
    "        return self.__X\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self.__y\n",
    "    \n",
    "    @X.setter\n",
    "    def X(self, X):\n",
    "        self.__X = X\n",
    "#         if X.shape[0] == len(y):\n",
    "#             self.__X = X\n",
    "#         else:\n",
    "#             raise ValueError('X debe tener la misma cantidad de filas que y')\n",
    "        \n",
    "    @y.setter\n",
    "    def y(self,y):\n",
    "        self.__y = y\n",
    "#         if X.shape[0] == len(y):\n",
    "#             self.__y = y\n",
    "#         else:\n",
    "#             raise ValueError('y debe tener la misma cantidad de filas que X')\n",
    "\n",
    "    \n",
    "    def get_posteriori(self, X, y, alpha, beta):\n",
    "        S_n_inv = alpha * np.eye(X.shape[1]) + beta * X.T.dot(X)\n",
    "        S_n = np.linalg.inv(S_n_inv)\n",
    "        m_n = beta * S_n.dot(X.T).dot(y)\n",
    "        return m_n, S_n\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Se verifican las dimensiones\n",
    "        if X.shape[0] == len(y):\n",
    "            # Se asocia al objeto la data con la que se realizó el fit\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "        else:\n",
    "            raise ValueError('la dimensión de y debe coincidir con la cantidad de filas que X')\n",
    "\n",
    "        #Se inicializan las iteraciones y el diff \n",
    "        iterations=0\n",
    "        diff = 2*self.tol\n",
    "        # Se obtienen los parámetros alfa,beta y se calcular m_n, S_n en funcion de éstos\n",
    "#         alfa, beta = self.get_params()['alpha'], self.get_params()['beta']\n",
    "        alfa = self.alpha\n",
    "        beta = self.beta\n",
    "        propios=[np.real_if_close(val) for val in np.linalg.eig(beta*X.T.dot(X))[0]]\n",
    "        \n",
    "        while (diff >= self.tol) and (iterations <= self.maxiter):\n",
    "        \n",
    "            m_n, S_n = self.get_posteriori(X, y, alfa, beta)\n",
    "        \n",
    "            # Se calcula gamma en función de los valores anteriores\n",
    "#             matrix = beta*X.T.dot(X)\n",
    "#             [val/(alpha+val) for val in np.linalg.eig(matrix)][0]\n",
    "            gamma = np.sum([val/(alpha+val) for val in propios])\n",
    "            \n",
    "            # Se calculan los nuevos alfa y beta\n",
    "            new_alfa = gamma / (m_n.T.dot(m_n))\n",
    "            \n",
    "            new_beta = 1/(1/(X.shape[0]-gamma) * np.sum([(y[i]-m_n.T.dot(X[i,:]))**2 for i in range(X.shape[0])]))\n",
    "            \n",
    "            # Se guarda al cambio de los parámetros en norma l2\n",
    "            diff = np.max([abs(alfa-new_alfa), abs(beta-new_beta)])\n",
    "            \n",
    "            # Se muestra en pantalla el estado actual\n",
    "            if iterations%50==0:\n",
    "                print('Iteración {}:\\nalpha = {}\\nbeta = {}\\n'.format(iterations,new_alfa,new_beta))\n",
    "            \n",
    "            # Se fijan los nuevos parámetros\n",
    "            #self.set_params(alpha=new_alfa, beta=new_beta)\n",
    "            alfa = new_alfa\n",
    "            beta = new_beta\n",
    "                \n",
    "            # Se sigue iterando\n",
    "            iterations+=1\n",
    "        \n",
    "        self.alpha = alfa\n",
    "        self.beta = beta\n",
    "        print('Fit Terminado en la iteración {}, con diferencias entre actualizaciones (en norma 2) {}'.format(iterations,diff))\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X_,return_std=False):\n",
    "        m_n, S_n = self.get_posteriori(self.X,self.y,self.alpha,self.beta)\n",
    "        \n",
    "        y_ = X_.dot(m_n)\n",
    "        #y_std = 1/beta + x.T.dot(S_n).dot(x)\n",
    "        y_std = []\n",
    "        for i in range(len(X_)):\n",
    "            x = X_[i]\n",
    "            sigma2_n = 1/self.beta + x.T.dot(S_n).dot(x)\n",
    "            y_std.append(np.sqrt(sigma2_n))\n",
    "        \n",
    "        if return_std:\n",
    "            return y_, y_std\n",
    "        else:\n",
    "            return y_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:39.095124Z",
     "start_time": "2020-05-20T03:17:38.833390Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Comenzamos realizando un test de normalidad sobre las variables numéricas dado que los valóres más extremos fueron\n",
    "removidos por los argumentos presentados en anteriores preguntas. De esta forma se decidirá cuales de las variables numéricas\n",
    "con aptas para usar StandardScaler.'''\n",
    "\n",
    "from scipy.stats import normaltest as nt\n",
    "print(df_raw['categorical'].columns)\n",
    "s, p = nt(df_raw['numeric'], axis=0)\n",
    "p<0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:39.159244Z",
     "start_time": "2020-05-20T03:17:39.101932Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Creamos una copia de la data a trabajar, dropeando las variables miscelaneas \n",
    "junto con `price`, `surface`, `property_type` y `upz`.\n",
    "Además dropeamos el nivel más externo del multiíndice por comodidad.'''\n",
    "\n",
    "d = df_raw.drop(columns='miscelaneous', level=0).drop(columns=['price', 'surface', 'property_type', 'upz'],level=1).copy()\n",
    "d.columns = d.columns.droplevel()\n",
    "d.n_garajes.unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:39.224819Z",
     "start_time": "2020-05-20T03:17:39.162590Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Definimos ahora listas de variables acorde al tratamiento a darles. Dados los resultados del test de normalidad,\n",
    "todas las variables serán consideradas NO provenientes de una normal. Es por esta razón que no se hará uso del objeto \n",
    "StandardScaler.\"\"\"\n",
    "categorical_vars = ['product_type','rent_type', 'upz_cluster', 'furnished']\n",
    "\n",
    "ordinal_vars = ['n_rooms', 'n_bath', 'n_garajes']\n",
    "ordinal_categories = [['1', '2', '3', '4', '5', '5+'],\n",
    "                      ['1', '2', '3', '4', '5', '5+'],\n",
    "                      ['1', '2', '3', '4', '4+']\n",
    "                     ]\n",
    "numeric_vars = ['UPlArea', 'metrocuadrado_index', 'personas',\n",
    "       'trabajoinf_ninos_5_17_anos_perc',\n",
    "       'trabajoinfampliado_ninos_5_17_anos_perc',\n",
    "       'jovenes_14_24_anos_nini_perc', 'indice_envegecimiento',\n",
    "       'jefe_mujer_perc', 'adultos_mayores_pobres_perc', 'indice_inseguridad',\n",
    "       'areas_verdes_perc', 'densidad_poblacion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:39.352591Z",
     "start_time": "2020-05-20T03:17:39.228358Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se hacen los imports necesarios para las transformaciones\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:39.496058Z",
     "start_time": "2020-05-20T03:17:39.356463Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se generan los pipelines para los distintos tipos de variables\n",
    "\n",
    "# pipeline variables categóricas\n",
    "pipe_categorico = Pipeline(steps=[('uno_caliente_codificador', OneHotEncoder(sparse=False))])\n",
    "\n",
    "# pipeline variables ordinales\n",
    "pipe_ordinal = Pipeline(steps=[('imputador_ordinal', SimpleImputer(strategy='most_frequent')),\n",
    "                              ('ordinal_codificador', OrdinalEncoder(categories=ordinal_categories))])\n",
    "\n",
    "# pipeline numéricas (todas NO provenientes de una normal)\n",
    "pipe_numerico = Pipeline(steps=[('min_max_escalador', MinMaxScaler()),\n",
    "                               ('poly_features', PolynomialFeatures(degree=3))])\n",
    "\n",
    "# pipeline final\n",
    "pipe_shishigang = ColumnTransformer(transformers=[\n",
    "    ('pipe_categotico', pipe_categorico, categorical_vars),\n",
    "    ('pipe_ordinal', pipe_ordinal, ordinal_vars),\n",
    "    ('pipe_numerico', pipe_numerico, numeric_vars)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:39.926929Z",
     "start_time": "2020-05-20T03:17:39.501116Z"
    }
   },
   "outputs": [],
   "source": [
    "X = d.drop(columns='price_per_m2').copy()\n",
    "X_t = pipe_shishigang.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:39.939754Z",
     "start_time": "2020-05-20T03:17:39.929602Z"
    }
   },
   "outputs": [],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:40.350883Z",
     "start_time": "2020-05-20T03:17:39.942690Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Probemos la P1 con lo anterior'''\n",
    "y_t=d['price_per_m2'].values\n",
    "estimador=RegresionBayesianaEmpirica(maxiter=50)\n",
    "estimador.fit(X_t[:100],y_t[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:40.391976Z",
     "start_time": "2020-05-20T03:17:40.354101Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred,y_std=estimador.predict(X_t[100:110],return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:04:23.899913Z",
     "start_time": "2020-05-20T03:04:23.892897Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:40.610685Z",
     "start_time": "2020-05-20T03:17:40.396989Z"
    }
   },
   "outputs": [],
   "source": [
    "ColumnTransformer(transformers=[\n",
    "    ('pipe_categotico', pipe_categorico, categorical_vars),\n",
    "    ('pipe_ordinal', pipe_ordinal, ordinal_vars),\n",
    "    ('pipe_numerico', pipe_numerico, numeric_vars)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:19:39.466175Z",
     "start_time": "2020-05-20T03:19:39.439769Z"
    }
   },
   "outputs": [],
   "source": [
    "X = d[(d['price_per_m2']>1)&(d['price_per_m2']<10**5)].drop(columns='price_per_m2').copy()\n",
    "y = d[(d['price_per_m2']>1)&(d['price_per_m2']<10**5)].price_per_m2.values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:19:57.802132Z",
     "start_time": "2020-05-20T03:19:57.775744Z"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:20:15.602429Z",
     "start_time": "2020-05-20T03:20:15.593670Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('transformador', pipe_shishigang), ('regresor', RegresionBayesianaEmpirica(alpha=10**(-7),beta=10**(-5)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:17:41.156172Z",
     "start_time": "2020-05-20T03:17:41.043106Z"
    }
   },
   "outputs": [],
   "source": [
    "#pipe['regresor'].predict(X_t[-645:-644])\n",
    "# x=X_t[-645:-644][0]\n",
    "# mean = m_n.T.dot(x)\n",
    "# sigma2_n = 1/beta + x.T.dot(S_n).dot(x)\n",
    "# print(sigma2_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:19:21.465100Z",
     "start_time": "2020-05-20T03:19:21.455164Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:20:33.991750Z",
     "start_time": "2020-05-20T03:20:33.224354Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:20:52.245491Z",
     "start_time": "2020-05-20T03:20:52.229964Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe['regresor'].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:21:52.844009Z",
     "start_time": "2020-05-20T03:21:52.374761Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred, y_std = pipe.predict(X_test,return_std=True)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rms = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:22:11.107090Z",
     "start_time": "2020-05-20T03:22:11.071511Z"
    }
   },
   "outputs": [],
   "source": [
    "np.std(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:22:30.823118Z",
     "start_time": "2020-05-20T03:22:30.059591Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:22:30.845945Z",
     "start_time": "2020-05-20T03:22:30.827099Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sum([x<0 for x in y_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:48:15.385919Z",
     "start_time": "2020-05-20T02:48:15.375946Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 422,
   "position": {
    "height": "444px",
    "left": "880px",
    "right": "20px",
    "top": "15px",
    "width": "307px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
