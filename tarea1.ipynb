{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MA6202: Laboratorio de Ciencia de Datos\n",
    "\n",
    "#### Profesor: Nicolás Caro\n",
    "\n",
    "#### Fecha de entrega: 17/05/2020\n",
    "\n",
    "#### Integrantes: Matías Romero, Danner Schlotterbeck, Kurt Walsen\n",
    "\n",
    "# Tarea 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:40.226019Z",
     "start_time": "2020-05-21T02:23:38.110679Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P1 Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:40.411522Z",
     "start_time": "2020-05-21T02:23:40.229012Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se carga una de las semanas de la data para inspeccionar:\n",
    "w13_all = pd.read_csv('./data/raw/w{}/metrocuadrado_all_w{}.csv'.format(13,13))\n",
    "w13_fur= pd.read_csv('./data/raw/w{}/metrocuadrado_furnished_w{}.csv'.format(13,13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:40.429474Z",
     "start_time": "2020-05-21T02:23:40.415512Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inspeccionamos sus columnas, y notamos que son las mismas\n",
    "print(w13_all.columns)\n",
    "print(w13_fur.columns)\n",
    "w13_all.columns==w13_fur.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos una concatenación con y sin duplicados para verificar la existencia de éstos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:40.607000Z",
     "start_time": "2020-05-21T02:23:40.432466Z"
    }
   },
   "outputs": [],
   "source": [
    "df_con=pd.concat([w13_all,w13_fur],ignore_index=True)\n",
    "df_con.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:40.769696Z",
     "start_time": "2020-05-21T02:23:40.609992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Verificamos si existen duplicados\n",
    "df_con.shape[0]==df_con.drop_duplicates().shape[0] # False, luego existen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscamos generar una variable categórica que indique si la observación correspondiente proviene de un archivo que en su nombre contiene `furnished`. Para ello haremos uso de la función merge.\n",
    "\n",
    "Notamos que la columna que sirve de identificador es `url` pues debiese ser único para cada propiedad en arriendo. Además podemos utilizarlo como llave para hacer el merge de los dataframes y ver qué ocurre con al columna `furnished`.\n",
    "\n",
    "De esta manera identificamos a archivos de texto `furnished` que no estén contenidos en archivos con texto `all` mediante la inspección de los valores right_only en la columna `furnished`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:40.982359Z",
     "start_time": "2020-05-21T02:23:40.772689Z"
    }
   },
   "outputs": [],
   "source": [
    "w13_all = w13_all.drop_duplicates()\n",
    "w13_fur = w13_fur.drop_duplicates()\n",
    "\n",
    "\n",
    "df_mer = pd.merge(left=w13_all,right=w13_fur,on='url',how='outer',indicator='furnished')\n",
    "df_mer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para w13 podemos notar 5 de éstos casos. Pero hay que verlo para el caso general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:41.074112Z",
     "start_time": "2020-05-21T02:23:40.986346Z"
    }
   },
   "outputs": [],
   "source": [
    "fur=df_mer.drop_duplicates()[['url','furnished']]\n",
    "fur[fur['furnished']=='right_only']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, agregamos al dataframe de concatenación la columna con la variable categórica furnished, mediante el uso de merge nuevamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:41.272587Z",
     "start_time": "2020-05-21T02:23:41.083087Z"
    }
   },
   "outputs": [],
   "source": [
    "df_con = df_con.drop_duplicates()\n",
    "pd.merge(df_con,fur,on='url').drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se compara con la data sin la columna agregada, se observa que poseen la misma cantidad de registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:41.322448Z",
     "start_time": "2020-05-21T02:23:41.280560Z"
    }
   },
   "outputs": [],
   "source": [
    "df_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:41.472051Z",
     "start_time": "2020-05-21T02:23:41.331425Z"
    }
   },
   "outputs": [],
   "source": [
    "w13_fur.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:41.614665Z",
     "start_time": "2020-05-21T02:23:41.478033Z"
    }
   },
   "outputs": [],
   "source": [
    "w13_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:41.759279Z",
     "start_time": "2020-05-21T02:23:41.616661Z"
    }
   },
   "outputs": [],
   "source": [
    "# Para unificar formatos, pasamos las columnas 'n_rooms' y 'n_bath' de _fur a string.\n",
    "w13_fur.n_rooms=w13_fur.n_rooms.astype(str)\n",
    "w13_fur.n_bath=w13_fur.n_bath.astype(str)\n",
    "w13_fur.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, realizamos el proceso general para todas las semanas disponibles en la data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:42.797502Z",
     "start_time": "2020-05-21T02:23:41.764266Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se concatenan todos los _all\n",
    "df_all = pd.concat([pd.read_csv('./data/raw/w{}/metrocuadrado_all_w{}.csv'.format(i,i)) for i in range(13,18)])\n",
    "print('Número de registros en df_all: '+str(len(df_all)))\n",
    "df_all.drop_duplicates(inplace=True)\n",
    "print('Número de registros en df_all sin duplicados: '+str(len(df_all)))\n",
    "\n",
    "# Se concatenan todos los _fur\n",
    "df_fur = pd.concat([pd.read_csv('./data/raw/w{}/metrocuadrado_furnished_w{}.csv'.format(i,i)) for i in range(13,18)])\n",
    "print('Número de registros en df_fur: '+str(len(df_fur)))\n",
    "df_fur.drop_duplicates(inplace=True)\n",
    "df_fur.reset_index()\n",
    "print('Número de registros en df_fur sin duplicados: '+str(len(df_fur)))\n",
    "\n",
    "furnished_only = df_fur.query(\"url not in @df_all.url\")\n",
    "print(\"Observaciones de archivos con texto 'furnished' que no estén\\n contenidos en archivos con texto 'all': \",\n",
    "      len(furnished_only))\n",
    "# Se agregan los que estan solo en furnished\n",
    "df = pd.concat([df_all, df_fur.query(\"url not in @df_all.url\")],ignore_index=True)\n",
    "\n",
    "# Se crea columna dummy y se le asigna valor 1 a los que se encontraban en archivos furnished\n",
    "from_furnished = df.query(\"url in @df_fur.url\").index\n",
    "df.loc[:, 'furnished'] = np.zeros(len(df), dtype=int)\n",
    "df.loc[from_furnished, 'furnished'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:42.821439Z",
     "start_time": "2020-05-21T02:23:42.799496Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a limpiar parte de la data, esto incluye cambiar el formato de ciertas variables de interés. En particular:\n",
    "\n",
    "`price` : Se decidió expresar la columna de precios como un número flotante, para evitar posibles futuras complicaciones en la inferencia estadística.\n",
    "            \n",
    "`surface` : Se modificaron los valores para que representen el valor en metros cuadrados como número flotante.\n",
    "            \n",
    "`n_rooms` : Se decidió expresar la columna de número de habitaciones como str, pues debido a la forma de los valores en esta columna es posible categorizarlas en base a la cantidad de habitacioens que registra, de esta manera los datos del tipo '5+' serán parte de una única categoría.\n",
    "\n",
    "`n_bath` : Análogo a `n_rooms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:43.019907Z",
     "start_time": "2020-05-21T02:23:42.823432Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se actualiza la columna 'price'\n",
    "df['price'] = df['price'].str.replace('.','').str.strip('$').map(float)\n",
    "\n",
    "# Se actualiza la columna 'surface'\n",
    "df['surface'] = df['surface'].replace('m2', '', regex=True).map(float)\n",
    "\n",
    "# Se genera un diccionario de reemplazos\n",
    "repl_dic = {1.0:'1',2.0:'2',3.0:'3',4.0:'4',5.0:'5'}\n",
    "\n",
    "# Se actualizan las columnas 'n_rooms' y 'n_bath'\n",
    "df['n_rooms'] = df['n_rooms'].replace(repl_dic)\n",
    "df['n_bath'] = df['n_bath'].replace(repl_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que existen registros en los cuales `surface` es 0, lo cual puede traer problemas a futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:43.094707Z",
     "start_time": "2020-05-21T02:23:43.022899Z"
    }
   },
   "outputs": [],
   "source": [
    "zero_surf=df[df['surface'] == 0][['price','surface']]\n",
    "nonzero_surf=df[df['surface'] > 0][['price','surface']]\n",
    "print(\"Total de registros: \",len(df))\n",
    "print(\"Registros con 'surface'=0 : \",len(zero_surf))\n",
    "print(\"Registros con 'surface'>0 : \",len(nonzero_surf))\n",
    "zero_surf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego podemos considerar estos datos como faltantes, pues en la práctica no existen propiedades de 0 m2 con tales precios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:43.227353Z",
     "start_time": "2020-05-21T02:23:43.096702Z"
    }
   },
   "outputs": [],
   "source": [
    "# Actualizamos 'surface'\n",
    "df.loc[:,'surface'] = df['surface'].replace(float(0),np.nan)\n",
    "\n",
    "# Verificamos que el cambio se realizó correctamente\n",
    "df[df['surface'].isnull()][['price','surface']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a separar el contenido de la columna `property_type|rent_type|location` en tres nuevas columnas `property_type`, `rent_type` y `location`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:43.481698Z",
     "start_time": "2020-05-21T02:23:43.229347Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preparamos la columna a tratar\n",
    "newcols = df['property_type|rent_type|location'].str.split(pat= ',',expand=True)\n",
    "\n",
    "# Se generan las nuevas columnas\n",
    "df['location'] = newcols[1]\n",
    "df[['property_type','rent_type']]=newcols[0].str.lower().str.split(pat=' ',n=1,expand=True)\n",
    "df.loc[:,'rent_type'] = df['rent_type'].str.lstrip('en ')\n",
    "\n",
    "# Fijamos las columnas de df en orden\n",
    "cols = ['property_type', 'rent_type', 'location', 'price', 'n_rooms',\n",
    "        'n_bath', 'surface', 'details', 'url', 'metrocuadrado_index',\n",
    "        'furnished']\n",
    "\n",
    "# Actualizamos df\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a agregar las columnas 'price_per_m2' y 'n_garajes',donde:\n",
    "\n",
    "`price_per_m2` : Representa el precio por metro cuadrado.\n",
    "\n",
    "`n_garajes` : Representa el número de garajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:43.532536Z",
     "start_time": "2020-05-21T02:23:43.484665Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generamos la nueva columna 'price_per_m2' \n",
    "df.loc[:,'price_per_m2'] = df['price'] / df['surface']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para `n_garajes` fue un proceso más complejo. Notamos que el string asociado a cada elemento en `url` es de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:43.654210Z",
     "start_time": "2020-05-21T02:23:43.535528Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df['url'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, este no es el caso de todas las `url`. En particular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:43.781868Z",
     "start_time": "2020-05-21T02:23:43.656204Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df['url'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, una manera intuitiva de obtener el número de garajes es separar el string con el separador '-garajes' lo cual generará una lista de dos strings, luego podemos acceder al primer string de la lista y extraer el último elemento, recuperando así el número de garajes. Lo cual nos entrega los siguientes valores para `n_garajes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:43.930503Z",
     "start_time": "2020-05-21T02:23:43.783864Z"
    }
   },
   "outputs": [],
   "source": [
    "df['n_garajes'] = df['url'].map(lambda string: string.split(sep='-garajes')[0][-1])\n",
    "\n",
    "df['n_garajes'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que aparece un elemento de la forma '+', luego al igual que `n_rooms` y `n_bath`, existe una categoria que representa una cantidad superior a cierto valor, la intuición nos dice que tal categoria corresponde a '9+'. En búsqueda de generalizar la transformación(y así incluir éste tipo de categorías) se uso una lambda function distinta, dando como resultado lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:44.112093Z",
     "start_time": "2020-05-21T02:23:43.937486Z"
    }
   },
   "outputs": [],
   "source": [
    "df['n_garajes'] = df['url'].map(lambda string: string.split(sep='-garajes')[0][-1] if string.split(sep='-garajes')[0][-1] !='+' else string.split(sep='-garajes')[0][-2:])\n",
    "df['n_garajes'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ésto nos muestra que la única categoria de la forma antes descrita corresponde solamente a '4+', algo confuso pues ya existen las categorías 5:9. Notamos que la cantidad de datos que corresponden a éste tipo de categoría son 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:44.242341Z",
     "start_time": "2020-05-21T02:23:44.119105Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(df['n_garajes'][df['n_garajes']=='4+']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora cúantos corresponden a las categorías >4, notamos que corresponden a 1010(bastante mayor en comparación a '4+')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:44.389972Z",
     "start_time": "2020-05-21T02:23:44.252311Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(df['n_garajes'][df['n_garajes']>'4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, notamos que la cantidad de datos que pueden representarse en la categoría '4+' corresponden a 1010(notar que al computar '4+'>'4' nos entrega True), una cifra bastante menor a los 16299 datos totales de los cuales disponemos, más precisamente, tenemos que las variables que superan la categoría '4' corresponden en promedio a 1010/6=168.3 por categoría, mientras que las variables en categoría '4' o inferior corresponden en promedio a (16299-1010)/5=3057.8 por categoría. En base a ésto, es correcto aseverar que podemos agrupar las variables con categoria superior a '4' en una única categoría '4+', sin perder variabilidad en el feature 'n_garajes'. Por lo tanto, generamos un diccionario que mapee todos las categorias resultantes superiores a '4' en una única categoría '4+'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:44.547553Z",
     "start_time": "2020-05-21T02:23:44.395955Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se genera diccionario de mapeos\n",
    "map_dict={'+':'4+', '5':'4+', '6':'4+', '7':'4+', '8':'4+', '9':'4+'}\n",
    "\n",
    "# Se genera la correcta columna 'n_garajes'\n",
    "df['n_garajes'] = df['url'].map(lambda string: string[\n",
    "    string.find('-garajes')-1] if string.find('-garajes') > 0 else np.nan )\n",
    "df['n_garajes'] = df['n_garajes'].replace(map_dict)\n",
    "df['n_garajes'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:44.696177Z",
     "start_time": "2020-05-21T02:23:44.550542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fijamos el orden de las columnas en df.\n",
    "cols = ['property_type', 'rent_type', 'location', 'price','price_per_m2', \n",
    "        'n_rooms', 'n_bath', 'n_garajes','surface', 'details', 'url', \n",
    "        'metrocuadrado_index', 'furnished']\n",
    "\n",
    "# Reordenamos df.\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:44.844755Z",
     "start_time": "2020-05-21T02:23:44.698145Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos categorizar los productos disponibles en la data según el tipo de inmueble al que corresponde y la cantidad de m2 de superficie que poseen, para ello haremos uso de 8 categorias:\n",
    "\n",
    "1 : `rent_type` = `casa` , 80 <= `surface` < 120\n",
    "\n",
    "2 : `rent_type` = `casa` , 120 <= `surface` < 180\n",
    "\n",
    "3 : `rent_type` = `casa` , 180 <= `surface` < \n",
    "\n",
    "4 : `rent_type` = `casa` , 240 <= `surface` < 360\n",
    "\n",
    "5 : `rent_type` = `casa` , 360 <= `surface` < \n",
    "\n",
    "6 : `rent_type` = `apartamento` , 40 <= `surface` < 60\n",
    "\n",
    "7 : `rent_type` = `apartamento` , 60 <= `surface` < 80\n",
    "\n",
    "8 : `rent_type` = `apartamento` , 80 <= `surface` < 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:45.063170Z",
     "start_time": "2020-05-21T02:23:44.846749Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se crea columna para ser rellenada a posteriori\n",
    "df['product_type'] = np.repeat([np.nan], len(df))\n",
    "\n",
    "# Se rellenan los tipos para las casas\n",
    "cotas_casas = [(80, 120), (120, 180), (180, 240), (240, 360), \n",
    "               (360,460)]\n",
    "\n",
    "for i in range(len(cotas_casas)):\n",
    "    q = \"(property_type == 'casa') & ({0} < surface <= {1})\".format(*cotas_casas[i])\n",
    "    idx = df.query(q).index\n",
    "    df.loc[idx, 'product_type'] = str(i+1)\n",
    "\n",
    "# Se rellenan los tipos para apartamentos    \n",
    "cotas_apartamentos = [(40, 60), (60, 80), (80, 120)]\n",
    "\n",
    "for i in range(len(cotas_apartamentos)):\n",
    "    q = \"(property_type == 'apartamento') & ({0} < surface <= {1})\".format(*cotas_apartamentos[i])\n",
    "    idx = df.query(q).index\n",
    "    df.loc[idx, 'product_type'] = str(i+(len(cotas_casas)+1))\n",
    "    \n",
    "print('Cantidad de productos no clasificados: '\n",
    "      ,len(df[df['product_type'].isna()]))\n",
    "print('Categorías: ',df['product_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:45.155923Z",
     "start_time": "2020-05-21T02:23:45.065164Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T01:15:15.139288Z",
     "start_time": "2020-05-14T01:15:15.109933Z"
    }
   },
   "source": [
    "## Parte 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos generar una nueva columna que indique el barrio, a partir de location. Notamos que todas las location poseen la estructuca '{barrio} Bogotá D.C.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:45.297543Z",
     "start_time": "2020-05-21T02:23:45.158913Z"
    }
   },
   "outputs": [],
   "source": [
    "df.location.unique()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos si hay locations que no contengan esta keyword, notamos que todos la tienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:45.421215Z",
     "start_time": "2020-05-21T02:23:45.300535Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df['location'].map(lambda string: not('Bogotá D.C.' in string))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a generar la columna `barrio`, de manera similar a como \n",
    "obtuvimos la columna `n_garajes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:45.635638Z",
     "start_time": "2020-05-21T02:23:45.423206Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df.index, 'barrio'] = df['location'].map(lambda string: \n",
    "                                                string.split(sep='Bogotá')[0].strip(' ').lower())\n",
    "print('Cantidad única de barrios disponibles: ',len(df['barrio'].unique()))\n",
    "df[['location','barrio']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:45.754323Z",
     "start_time": "2020-05-21T02:23:45.638630Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se examina barrio-upz.csv para determinar cómo hacer el merge\n",
    "upz=pd.read_csv('./data/asignacion_upz/barrio-upz.csv')\n",
    "upz.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:45.885971Z",
     "start_time": "2020-05-21T02:23:45.757314Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Cantidad única de UPlNombre disponibles: ',len(upz.UPlNombre.unique()))\n",
    "upz.UPlNombre.unique()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:46.023600Z",
     "start_time": "2020-05-21T02:23:45.887962Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Cantidad única de pro_location disponibles: ',len(upz.pro_location.unique()))\n",
    "upz.pro_location.unique()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que `pro_location` presenta una gama más amplia de barrios sobre los cuales poder cruzar la data. En base a esto, haremos un cruce entre df y asignacion_upz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:46.265953Z",
     "start_time": "2020-05-21T02:23:46.025595Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se realiza un merge 'outer' para determinar cuántos no tienen código upz\n",
    "df_merged = pd.merge(df,upz,left_on='barrio',right_on='pro_location',\n",
    "                     how='outer',indicator='ind')\n",
    "\n",
    "barrios_upz = df_merged[df_merged['ind']=='right_only']['UPlNombre'].nunique()\n",
    "obs_sin = len(df_merged[df_merged['ind']=='left_only'])\n",
    "print('Barrios con código UPZ que no están en df: '+str(barrios_upz))\n",
    "print('Observaciones sin código UPZ: '+str(obs_sin))\n",
    "print('Barrios sin código UPZ: '+str(df_merged[df_merged['ind']=='left_only'].barrio.nunique()))\n",
    "porc = int(100*len(df_merged[df_merged['ind']=='both'])/len(df))\n",
    "print('El {}% de las observaciones tienen código UPZ'.format(porc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que existen 13 barrios de la data upz los cuales no aparecen en nuestro df, ésto se quizás a que no existen publicaciones de arriendo/venta de propiedades pertenecientes a tales barrios. Además, se observa que 1946 registros pertenecen a barrios los cuales no se les puede adjuntar un código UPZ. De éstos notamos que la cantidad única de barrios corresponde a 176. Además, un 88% de las observaciones en df poseen\n",
    "un código UPZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:46.325791Z",
     "start_time": "2020-05-21T02:23:46.267946Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se define el nuevo df\n",
    "df = df_merged[df_merged['ind']!='right_only'].drop(columns=['UPlTipo','UPlNombre','ind','pro_location'])\n",
    "\n",
    "df.reset_index(drop=True)\n",
    "\n",
    "# Fijamos las columnas en df.\n",
    "cols = ['product_type','property_type', 'rent_type', 'location','barrio','UPlCodigo',\n",
    "        'UPlArea', 'price','price_per_m2','surface', 'n_rooms', 'n_bath',\n",
    "        'n_garajes', 'details', 'url', 'metrocuadrado_index', 'furnished']\n",
    "\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:46.472401Z",
     "start_time": "2020-05-21T02:23:46.327786Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:46.720770Z",
     "start_time": "2020-05-21T02:23:46.474394Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cargamos la data asociada a estadisticas_poblacion.csv\n",
    "stats_pob=pd.read_csv('./data/estadisticas_upz/estadisticas_poblacion.csv')\n",
    "stats_pob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:46.933170Z",
     "start_time": "2020-05-21T02:23:46.722730Z"
    }
   },
   "outputs": [],
   "source": [
    "# Estructura columna de UPlCodigo para la data stats_pob, a priori\n",
    "# no se observas complicaciones en el formato.\n",
    "stats_pob.upz.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:47.243342Z",
     "start_time": "2020-05-21T02:23:46.935162Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rescatamos solo las columnas relevantes\n",
    "stats_pob.drop(columns=['Unnamed: 0','nomupz'],inplace=True)\n",
    "df_merged = pd.merge(df,stats_pob,left_on='UPlCodigo',right_on='upz',how='left')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:47.344069Z",
     "start_time": "2020-05-21T02:23:47.246330Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cargamos la data asociada a indice_inseguridad.csv\n",
    "ind_inseg=pd.read_csv('./data/estadisticas_upz/indice_inseguridad.csv')\n",
    "ind_inseg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:47.503644Z",
     "start_time": "2020-05-21T02:23:47.346063Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notamos que hay códigos '1', '3', '4' y '5', pero también notamos\n",
    "# que éstos UPZ{} no están en df ni coinciden con otros, por lo que \n",
    "# los ignoramos.\n",
    "print('Códigos UPZ en ind_inseg: ',ind_inseg.UPlCodigo.unique())\n",
    "ind_inseg.drop(columns=['Unnamed: 0','UPlNombre2'],inplace=True)\n",
    "df_merged = pd.merge(df_merged,ind_inseg,on='UPlCodigo',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:48.058160Z",
     "start_time": "2020-05-21T02:23:47.506635Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:48.163877Z",
     "start_time": "2020-05-21T02:23:48.060155Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cargamos la data asociada a porcentaje_areas_verdes.csv\n",
    "perc_areas_verdes = pd.read_csv('./data/estadisticas_upz/porcentaje_areas_verdes.csv')\n",
    "perc_areas_verdes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:48.333650Z",
     "start_time": "2020-05-21T02:23:48.165869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Estructura columna de UPlCodigo para la data perc_areas_verdes,\n",
    "# notamos que contiene solo floats, sin el prefijo 'UPZ'\n",
    "print('Previo a la transformación:\\n',perc_areas_verdes['cod_upz'].unique())\n",
    "# Se tranforma x en str de la forma 'UPZ'+str(int(x))\n",
    "perc_areas_verdes.loc[:,'cod_upz']=perc_areas_verdes['cod_upz'].map(lambda x: 'UPZ'+str(int(x)))\n",
    "print('Luego de la transformación:\\n',perc_areas_verdes['cod_upz'].unique())\n",
    "\n",
    "# Se dropean columnas innecesarias y se hace un último merge\n",
    "perc_areas_verdes.drop(columns=['Unnamed: 0','upz'],inplace=True)\n",
    "df_merged = pd.merge(df_merged,perc_areas_verdes,left_on='UPlCodigo',right_on='cod_upz',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:48.581984Z",
     "start_time": "2020-05-21T02:23:48.337639Z"
    }
   },
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:48.868219Z",
     "start_time": "2020-05-21T02:23:48.583978Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hay 3 columnas que dicen lo mismo: 'UPlCodigo','upz','cod_upz'. Nos quedamos con la segunda por simplicidad\n",
    "df=df_merged.drop(columns=['UPlCodigo','cod_upz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:49.037767Z",
     "start_time": "2020-05-21T02:23:48.871211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generamos la nueva columna con la densidad de población por UPZ\n",
    "df['densidad_poblacion']=df['personas']/df['UPlArea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:49.314027Z",
     "start_time": "2020-05-21T02:23:49.040758Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Botamos los últimos duplicados\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:49.321009Z",
     "start_time": "2020-05-21T02:23:49.316022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se guarda una copia de la data procesada hasta ahora\n",
    "#df_raw = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:49.498533Z",
     "start_time": "2020-05-21T02:23:49.324000Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creamos una función que fije cómo queremos que se vean los plots.\n",
    "def estilo():\n",
    "    sns.set(style='darkgrid')\n",
    "    plt.rcParams['figure.figsize'] = (18, 18)\n",
    "estilo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscamos hacer un perfilamiento de las variables disponibles en la data a partir de la parte anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:49.680047Z",
     "start_time": "2020-05-21T02:23:49.500527Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:49.815686Z",
     "start_time": "2020-05-21T02:23:49.682042Z"
    }
   },
   "outputs": [],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:49.979248Z",
     "start_time": "2020-05-21T02:23:49.817679Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tipos de variables que queremos\n",
    "names = ['numeric','categorical','miscelaneous']\n",
    "\n",
    "# Fijamos las variables numéricas\n",
    "numeric = ['UPlArea','price','surface','metrocuadrado_index',\n",
    "           'personas', 'trabajoinf_ninos_5_17_anos_perc',\n",
    "           'trabajoinfampliado_ninos_5_17_anos_perc',\n",
    "           'jovenes_14_24_anos_nini_perc','indice_envegecimiento',\n",
    "           'jefe_mujer_perc','adultos_mayores_pobres_perc','indice_inseguridad',\n",
    "           'areas_verdes_perc','densidad_poblacion','price_per_m2']\n",
    "\n",
    "# Fijamos las variables miscelaneas, recordemos que el barrio\n",
    "# puede identificarse con un código upz\n",
    "miscelaneous= ['location','barrio','url','details']\n",
    "\n",
    "# Se crea una lista con las variables categoricas\n",
    "categorical = list((set(df.columns) - set(numeric)) - set(miscelaneous))\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:50.146799Z",
     "start_time": "2020-05-21T02:23:49.981241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generamos el mapeo a multi-índices\n",
    "mapping = [('numeric', col) for col in numeric]\n",
    "mapping.extend([('categorical', col) for col in categorical])\n",
    "mapping.extend([('miscelaneous', col) for col in miscelaneous])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:50.370201Z",
     "start_time": "2020-05-21T02:23:50.148793Z"
    }
   },
   "outputs": [],
   "source": [
    "# Re-indexamos la data\n",
    "df = df.reindex(columns=numeric + categorical + miscelaneous)\n",
    "df.columns = pd.MultiIndex.from_tuples(mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardaremos una copia de df para poder hacer modificaciones en el camino y de esta manera entender mejor las visualizaciones. Más adelante veremos que tales modificaciones resultan útiles a la hora de poder obtener una mejor identificación entre las variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:50.441012Z",
     "start_time": "2020-05-21T02:23:50.372195Z"
    }
   },
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comienza perfilando las variables numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:59.672321Z",
     "start_time": "2020-05-21T02:23:50.443007Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se procede a graficar las distribuciones de las variables numéricas\n",
    "'''\n",
    "# Grilla de subplots\n",
    "fig, ax = plt.subplots(nrows=4, ncols=4)#, figsize=[17, 17])\n",
    "\n",
    "# Se remueven el ultimo plot\n",
    "list(map(lambda a : a.remove(), ax[-1,-1:]))\n",
    "\n",
    "# Se ajusta el espaciado exterior de la figura\n",
    "fig.tight_layout()\n",
    "\n",
    "# Se define un titulo y su ubicacion\n",
    "fig.suptitle('Distribuciones Univariadas Numéricas',\n",
    "             fontsize=20,\n",
    "             x=0.5,\n",
    "             y=1.05)\n",
    "'''\n",
    "Se recorre cada axis, para cada columna del dataframe, se genera un grafico \n",
    "distinto en funcion del tipo de dato.\n",
    "\n",
    "'''\n",
    "for axis, col in zip(ax.flatten(), numeric):\n",
    "    try :\n",
    "        # Graficos para datos numericos\n",
    "        sns.distplot(df[('numeric', col)].dropna(), ax=axis, rug=True)\n",
    "               \n",
    "    except RuntimeError:\n",
    "        sns.distplot(df[('numeric', col)].dropna(), ax=axis, rug=True, kde=False)\n",
    "    \n",
    "    axis.set_xlabel(col, fontsize=15)\n",
    "\n",
    "# Se ajusta el espaciado interno entre subplots\n",
    "w, h = (.4, .4)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos en `price` y `price per_m2` que hay una diferencia muy brusca en la densidad de los datos, donde la tendencia es clara a precios mas moderados en comparacion. Dado que `price_per_m2` es nuestra variable respuesta, nos interesa que ésta presente una distribución que sea lo suficientemente suave para poder realizar futuras transformaciones en nuestro futuro modelo regresor. Notamos que considerando datos con valor de `price_per_m2` < 100.000, se obtiene el siguiente dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:23:59.842863Z",
     "start_time": "2020-05-21T02:23:59.694260Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df[('numeric','price_per_m2')]<=10**5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos una diferencia de 100 datos con la data original, es decir, estamos ignorando los 100 valores donde `price_per_m2` > 100.000. Veamos ahora como se ve su distribución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:00.432288Z",
     "start_time": "2020-05-21T02:23:59.844856Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(df[df[('numeric','price_per_m2')] <= 10**5][('numeric','price_per_m2')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos una clara mejora con respecto al escenario con toda la data. Actualizamos df y vemos ahora como distribuyen todas las variables numericas asociadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:00.530027Z",
     "start_time": "2020-05-21T02:24:00.435277Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df[('numeric','price_per_m2')] <= 10**5]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:09.209480Z",
     "start_time": "2020-05-21T02:24:00.533016Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se procede a graficar las distribuciones de las variables numéricas\n",
    "luego del ajuste\n",
    "'''\n",
    "# Grilla de subplots\n",
    "fig, ax = plt.subplots(nrows=4, ncols=4)#, figsize=[17, 17])\n",
    "\n",
    "# Se remueven el ultimo plot\n",
    "list(map(lambda a : a.remove(), ax[-1,-1:]))\n",
    "\n",
    "# Se ajusta el espaciado exterior de la figura\n",
    "fig.tight_layout()\n",
    "\n",
    "# Se define un titulo y su ubicacion\n",
    "fig.suptitle('Distribuciones Univariadas Numéricas',\n",
    "             fontsize=20,\n",
    "             x=0.5,\n",
    "             y=1.05)\n",
    "'''\n",
    "Se recorre cada axis, para cada columna del dataframe, se genera un grafico \n",
    "distinto en funcion del tipo de dato.\n",
    "\n",
    "'''\n",
    "for axis, col in zip(ax.flatten(), numeric):\n",
    "    try :\n",
    "        # Graficos para datos numericos\n",
    "        sns.distplot(df[('numeric', col)].dropna(), ax=axis, rug=True)\n",
    "               \n",
    "    except RuntimeError:\n",
    "        sns.distplot(df[('numeric', col)].dropna(), ax=axis, rug=True, kde=False)\n",
    "    \n",
    "    axis.set_xlabel(col, fontsize=15)\n",
    "\n",
    "# Se ajusta el espaciado interno entre subplots\n",
    "w, h = (.4, .4)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos cómo la distribución de la data es más clara al trabajar sin los casos donde `price_per_m2` es demasiado grande. Notamos en `surface` que existe un dato que se aleja mucho de donde se concentra la data, y su valor es al menos superior a 1000, donde los demas valores se concentran por debajo de 1000. Buscamos tal valor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:09.221450Z",
     "start_time": "2020-05-21T02:24:09.211476Z"
    }
   },
   "outputs": [],
   "source": [
    "df[('numeric','surface')].nlargest(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué ocurre al quitar tal valor de la data, notamos una clara mejora en la visualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:09.859126Z",
     "start_time": "2020-05-21T02:24:09.224441Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(df[df[('numeric','surface')]< 10**3][('numeric','surface')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:09.876083Z",
     "start_time": "2020-05-21T02:24:09.861121Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se actualiza df\n",
    "df = df[df[('numeric','surface')]< 10**3]\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:19.020897Z",
     "start_time": "2020-05-21T02:24:09.892040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grilla de subplots\n",
    "fig, ax = plt.subplots(nrows=4, ncols=4)#, figsize=[17, 17])\n",
    "\n",
    "# Se remueven el ultimo plot\n",
    "list(map(lambda a : a.remove(), ax[-1,-1:]))\n",
    "\n",
    "# Se ajusta el espaciado exterior de la figura\n",
    "fig.tight_layout()\n",
    "\n",
    "# Se define un titulo y su ubicacion\n",
    "fig.suptitle('Distribuciones Univariadas Numéricas',\n",
    "             fontsize=20,\n",
    "             x=0.5,\n",
    "             y=1.05)\n",
    "'''\n",
    "Se recorre cada axis, para cada columna del dataframe, se genera un grafico \n",
    "distinto en funcion del tipo de dato.\n",
    "\n",
    "'''\n",
    "for axis, col in zip(ax.flatten(), numeric):\n",
    "    try :\n",
    "        # Graficos para datos numericos\n",
    "        sns.distplot(df[('numeric', col)].dropna(), ax=axis, rug=True)\n",
    "               \n",
    "    except RuntimeError:\n",
    "        sns.distplot(df[('numeric', col)].dropna(), ax=axis, rug=True, kde=False)\n",
    "    \n",
    "    axis.set_xlabel(col, fontsize=15)\n",
    "\n",
    "# Se ajusta el espaciado interno entre subplots\n",
    "w, h = (.4, .4)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos ver como se comporta la variable `price_per_m2` en respuesta a algunas variables numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:19.039853Z",
     "start_time": "2020-05-21T02:24:19.027852Z"
    }
   },
   "outputs": [],
   "source": [
    "def scatter_dists(col, df=df, h=.3, w=.1, fontdict={'fontsize': 20}, reg=True):\n",
    "    ''' Recibe una columna numerica y genera una visualizacion comparativa.\n",
    "    \n",
    "    Genera una figura por sobre el dataframe (por defecto), recibe \n",
    "    parametros extra como el espaciado entre subfigura.\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "    \n",
    "    col: String\n",
    "         El nombre de la columna numerica a visualizar\n",
    "    \n",
    "    h,w: float\n",
    "        Espaciado entre subplot h -> vertical, w -> horizontal\n",
    "    \n",
    "    fontdict: dict\n",
    "             Permite configurar las fuentes de los subplots\n",
    "    reg: bool\n",
    "         Permite graficar una regresion lineal sobre los datos (if True)\n",
    "        \n",
    "    Returns: None\n",
    "        Se muestra una figura en pantalla    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Estrucutra de figura y axes\n",
    "    fig, ax = plt.subplots(2, 1, figsize=[12, 13])\n",
    "\n",
    "    # violin plot --> equivalente a catplot(kind = 'violin')\n",
    "\n",
    "    if reg:\n",
    "        sns.regplot(x=df[('numeric', col)],\n",
    "                    y=df[('numeric', 'price_per_m2')],\n",
    "                    ax=ax[0])\n",
    "        ax[0].set_title('Regplot plot {} vs price_per_m2'.format(col), fontdict)\n",
    "    else:\n",
    "        sns.scatterplot(('numeric', col),\n",
    "                        y=('numeric', 'price_per_m2'),\n",
    "                        data=df,\n",
    "                        ax=ax[0])\n",
    "        ax[0].set_title('Scatter plot {} vs price_per_m2'.format(col), fontdict)\n",
    "\n",
    "    \n",
    "    # Distribucion univariada\n",
    "    sns.distplot(df[('numeric', col)].dropna(), ax=ax[1])\n",
    "\n",
    "    ax[0].set_xlabel(col, fontdict)\n",
    "    ax[1].set_xlabel(col, fontdict)\n",
    "\n",
    "    ax[0].set_ylabel('price_per_m2', fontdict)\n",
    "    ax[1].set_title('Frecuencias {}'.format(col), fontdict)\n",
    "\n",
    "    plt.subplots_adjust(wspace=w, hspace=h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que la variable `metrocuadrado_index`, presenta una buena distribución en la data y presenta un comportamiento lineal con ruido, puede que esta variable sea de interés a la hora de regresionar `price_per_m2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:20.884299Z",
     "start_time": "2020-05-21T02:24:19.041817Z"
    }
   },
   "outputs": [],
   "source": [
    "scatter_dists('metrocuadrado_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que las variables `price` y `surface` presentan un comportamiento lineal con respecto a `price_per_m2`, pero esto se debe a como se generó la columna `price_per_m2`, luego la relación existente entre éstas variables fue impuesta y no presenta un caso de interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:22.417198Z",
     "start_time": "2020-05-21T02:24:20.886294Z"
    }
   },
   "outputs": [],
   "source": [
    "scatter_dists('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:24.108677Z",
     "start_time": "2020-05-21T02:24:22.419192Z"
    }
   },
   "outputs": [],
   "source": [
    "scatter_dists('surface')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruido, `price_per_m2` no presenta ninguna respuesta clara ante esta variable. Su significancia deberá ser evaluada más adelante.\n",
    "Resultado análogo entre:\n",
    "\n",
    "`UPlArea`\n",
    "\n",
    "`trabajoinf_ninos_5_17_anos_perc`\n",
    "\n",
    "`trabajoinfampliado_ninos_5_17_anos_perc`\n",
    "\n",
    "`jovenes_14_24_anos_nini_perc`\n",
    "\n",
    "`indice_envegecimiento`\n",
    "\n",
    "`jefe_mujer_perc`\n",
    "\n",
    "`adultos_mayores_pobres_perc`\n",
    "\n",
    "`indice_inseguridad`\n",
    "\n",
    "`areas_verdes_perc`\n",
    "\n",
    "`densidad_poblacion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:25.963712Z",
     "start_time": "2020-05-21T02:24:24.113662Z"
    }
   },
   "outputs": [],
   "source": [
    "scatter_dists('UPlArea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a analizar las variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:25.973684Z",
     "start_time": "2020-05-21T02:24:25.966703Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(categorical))\n",
    "print(categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:29.371564Z",
     "start_time": "2020-05-21T02:24:25.976677Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se procede a graficar los histogramas de las variables categóricas\n",
    "'''\n",
    "# Grilla de subplots\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3)\n",
    "\n",
    "# Se remueven el ultimo plot\n",
    "list(map(lambda a : a.remove(), ax[-1,-1:]))\n",
    "\n",
    "# Se ajusta el espaciado exterior de la figura\n",
    "fig.tight_layout()\n",
    "# Se define un titulo y su ubicacion\n",
    "fig.suptitle('Distribuciones Univariadas Categóricas',\n",
    "             fontsize=20,\n",
    "             x=0.5,\n",
    "             y=1.05)\n",
    "'''\n",
    "Se recorre cada axis, para cada columna del dataframe, se genera un grafico \n",
    "distinto en funcion del tipo de dato.\n",
    "\n",
    "'''\n",
    "for axis, col in zip(ax.flatten(), categorical):\n",
    "    # Graficos para datos tipos str\n",
    "    sns.countplot(df[('categorical',col)], ax=axis)\n",
    "    axis.set_axis_off()\n",
    "    axis.set_title(col, fontsize=15)\n",
    "  \n",
    "    \n",
    "# Se ajusta el espaciado interno entre subplots\n",
    "h, w = (.4, .1)\n",
    "plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:29.385558Z",
     "start_time": "2020-05-21T02:24:29.373560Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Estudiemos la variabilidad de las variables categóricas ''\n",
    "'''\n",
    "# Función para generar gráficos\n",
    "def categoricalplot(df,col,log=False):\n",
    "    # Sirve para fija el tamaño de las etiquetas del plot\n",
    "    fontdict = {'fontsize':20}\n",
    "\n",
    "    # Estrucutra de figura y axes\n",
    "    fig, ax = plt.subplots(2,1,figsize=[12,13])\n",
    "    \n",
    "    # violin plot --> equivalente a catplot(kind = 'violin')\n",
    "\n",
    "    if log:\n",
    "        sns.violinplot(('categorical', col),\n",
    "                    y=('numeric', 'price_per_m2'),\n",
    "                    data=df,\n",
    "                    kind='violin',\n",
    "                    ax=ax[0]).set_yscale('log')\n",
    "    \n",
    "    else:\n",
    "        sns.violinplot(('categorical', col),\n",
    "                    y=('numeric', 'price_per_m2'),\n",
    "                    data=df,\n",
    "                    kind='violin',\n",
    "                    ax=ax[0])\n",
    "    \n",
    "    sns.countplot(df[('categorical',col)], ax=ax[1])\n",
    "\n",
    "    ax[0].set_xlabel(col, fontdict)\n",
    "    ax[1].set_xlabel(col, fontdict)\n",
    "\n",
    "    ax[0].set_ylabel('price_per_m2', fontdict)\n",
    "    ax[0].set_title('Violin plot {} vs price_per_m2'.format(col), fontdict)\n",
    "    ax[1].set_title('Frecuencias {}'.format(col), fontdict)\n",
    "\n",
    "    h, w = (.3, .1)\n",
    "    plt.subplots_adjust(wspace=w, hspace=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`upz`\n",
    "\n",
    "Observamos que la variable categorica `upz` presenta un histograma a la vista confuso(demasiadas categorias), por lo que más adelante buscaremos una mejor manera de agrupar éstas categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:37.953496Z",
     "start_time": "2020-05-21T02:24:29.388519Z"
    }
   },
   "outputs": [],
   "source": [
    "categoricalplot(df,'upz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`product_type`\n",
    "\n",
    "Notamos que para los `product_type` 1-5 (casas), presentan una concentración en valores levemente más bajos que los 6-8 (apartamentos), además la variabilidad del valor de precio dado si es casa o apartamento presentan una distribución similar. Por ende `product_type` podría corresponder a una variable de interés a la hora de definir `price_per_m2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:38.571843Z",
     "start_time": "2020-05-21T02:24:37.955492Z"
    }
   },
   "outputs": [],
   "source": [
    "categoricalplot(df,'product_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`property_type`\n",
    "\n",
    "Notamos que los valores de `price_per_m2` para la categoría 'casa' se concentran en su mayoría en un valor menor que la categoría apartamento. Ésto se condice con el gráfico anterior y se debe en parte a cómo definimos la variable `product_type`, luego puede existir cierta correlación entre estas dos variables, lo cual puede corroborarse mediante un test estadístico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:39.053631Z",
     "start_time": "2020-05-21T02:24:38.574836Z"
    }
   },
   "outputs": [],
   "source": [
    "categoricalplot(df,'property_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`rent_type`\n",
    "\n",
    "Notamos que no difieren en mediana y sus distribuciones en `price_per_m2` se comportan de manera similar, por ende no existe una manera de poder identificar una de las categorías en base al valor de `price_per_m2`. Por lo tanto, `rent_type` corresponde a una variable candidata a no ser considerada en el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:39.456275Z",
     "start_time": "2020-05-21T02:24:39.056623Z"
    }
   },
   "outputs": [],
   "source": [
    "categoricalplot(df,'rent_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`furnished`\n",
    "\n",
    "Si bien no se nota una diferencia en los valores donde más se cocentra cada categoría, el hecho de la clara diferencia en la distribución de la variable nos hace dudar sobre la efectividad en describir la variable `price_per_m2`, en caso de ser incluída en el modelo. Por lo tanto, más adelante se verá si incluir o no esta variable en el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:40.147016Z",
     "start_time": "2020-05-21T02:24:39.459267Z"
    }
   },
   "outputs": [],
   "source": [
    "categoricalplot(df,'furnished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_rooms`\n",
    "\n",
    "Se notan claras diferencias en las medias del valor `price_per_m2` entre categorias, ademas de una distribución normalmente aproximable en los valores en esta variable. Por lo tanto corresponde a un candidato sólido a ser incluido en el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:40.720365Z",
     "start_time": "2020-05-21T02:24:40.149010Z"
    }
   },
   "outputs": [],
   "source": [
    "categoricalplot(df,'n_rooms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_bath`\n",
    "\n",
    "Si bien la diferencia de medias en este caso tambien existe pero en menor medida, la forma variabilidad en su distribución puede ser información valiosa para la descripción de `price_per_m2`, por lo tanto a priori es una variable interesante a considerar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:41.214064Z",
     "start_time": "2020-05-21T02:24:40.722360Z"
    }
   },
   "outputs": [],
   "source": [
    "categoricalplot(df,'n_bath')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_garajes`\n",
    "\n",
    "Tenemos un caso similar a `n_rooms`, donde aquí la diferencia de medias es menos clara, y debido a que los violines son más achatados nos hace dudar sobre la correcta descripción de `price_per_m2` por medio de esta variable. Veremos mediante un test one-way-ANOVA si existe una diferencia significativa entre grupos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:41.790523Z",
     "start_time": "2020-05-21T02:24:41.216061Z"
    }
   },
   "outputs": [],
   "source": [
    "categoricalplot(df,'n_garajes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:41.799500Z",
     "start_time": "2020-05-21T02:24:41.792518Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se genera una función auxiliar para indexar las columnas en base\n",
    "a su tipo, extraído de la clase 9 del curso.\n",
    "'''\n",
    "def indexer(cols, t_c = df.columns):\n",
    "    '''Genera columnas multinivel a partir de nombres de columna planos.'''\n",
    "    \n",
    "    set_to_tuple = set(*[cols])\n",
    "\n",
    "    tuples = [\n",
    "        i for i in t_c if set_to_tuple.intersection(set(i))\n",
    "    ]\n",
    "    \n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:41.979559Z",
     "start_time": "2020-05-21T02:24:41.802491Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = indexer(['price_per_m2','n_garajes'])\n",
    "grouped = df[idx].groupby(idx[1])\n",
    "total_groups = grouped.groups.keys()\n",
    "groups = [grouped.get_group(i) for i in total_groups]\n",
    "groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:42.135460Z",
     "start_time": "2020-05-21T02:24:41.981520Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se utiliza una función auxiliar para limpiar el formato de cada grupo,\n",
    "extraído de la clase 9 del curso.\n",
    "'''\n",
    "def group_cleaner(group, col, d_f=df):\n",
    "    ''' Limpia un grupo.\n",
    "    Reconoce la categoria del grupo, en la posicion [:,1], \n",
    "    guarda ese nombre y elimina la columna de categoria, \n",
    "    posteriormente renombra la columna.\n",
    "    \n",
    "    Args:\n",
    "    ----------\n",
    "    \n",
    "    group: pandas Groupby object\n",
    "          Recibe una agrupacion para categorias\n",
    "          \n",
    "    Returns:\n",
    "    ----------\n",
    "        pandas Groupby object\n",
    "        Entrega el grupo ordenado.\n",
    "    '''\n",
    "    group_0 = group.copy()\n",
    "    name = group_0.iloc[0,1]\n",
    "    group_0.drop(indexer([col], t_c = d_f.columns), axis=1, inplace=True)\n",
    "    group_0.columns  = ('cat_{}'.format(name),)\n",
    "    \n",
    "    return group_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:42.319076Z",
     "start_time": "2020-05-21T02:24:42.142477Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se procede a limpiar el formato de cada grupo y se realiza el test\n",
    "'''\n",
    "groups_to_test = [group_cleaner(g, 'n_garajes') for g in groups]\n",
    "\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "F,p = f_oneway(*groups_to_test)\n",
    "\n",
    "print('Estadistico F:',F)\n",
    "print('p valor :', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto, rechazamos la nula: 'No hay diferencia significativa entre grupos'. Por lo tanto, `n_garajes` corresponde a una variable de interés a analizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:42.427785Z",
     "start_time": "2020-05-21T02:24:42.324064Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "p <= alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver las variables faltantes, recuperamos df_copy previo a las modificaciones hechas en P2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:42.565419Z",
     "start_time": "2020-05-21T02:24:42.429779Z"
    }
   },
   "outputs": [],
   "source": [
    "df_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos un esquema general de los valores faltantes en de la data, se puede observar un comportamiento similar en la ausencia de los datos asociados a las estadísticas incluidas mediante un cruce con los códigos upz. Ésto claramente es debido a que al existir barrios donde no fue posible obtener identificación mediante el código upz, no fue posible cruzar las estadísticas en la sección P1.6, por lo tanto la ausencia de las estadísticas se refleja en la ausencia de upz. El tratamiento para estos datos faltantes será eliminarlos, pues debido a no poder recuperar el upz, no podremos recuperar de manera consistente las estadísticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:43.332366Z",
     "start_time": "2020-05-21T02:24:42.568409Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = [15, 10])\n",
    "msno.matrix(df_copy,ax = ax, sparkline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos un esquema ordenado según cantidad de datos faltantes. Observamos que las columnas con más datos faltantes correspondes a `product_type` y `n_garajes`\n",
    "\n",
    "`product_type` se debe a como la definimos en la sección P1.4, luego depende de parámetros visibles en la data (`property_type`,`surface`). Nuestro tratamiento para esta variable será simplemente eliminarlas.\n",
    "\n",
    "`n_garajes` se debe a la ausencia de la keyword '-garajes' en `url`. Por lo tanto, la ausencia de ésta data se puede inferir a partir de la variable `url`, variable sobre la cual fue construida esta columna.\n",
    "\n",
    "Respecto a las columnas `n_bath`,`details`,`n_rooms`,`price`,`surface` no es posible determinar un patron claro, más aun cuando la ausencia de las variables en las ultimas 3 mencionadas son pocas (menos de 33). Creemos entonces que ésta información es perdida completamente al azar, pues depende de algo que no estamos viendo reflejado en la data(mal ingreso de los datos, omisión de informacion por parte del vendedor,etc.). Ésta información se intentará imputar asignando media mediante agrupaciónes por `upz` donde sea posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:44.040503Z",
     "start_time": "2020-05-21T02:24:43.334360Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msno.matrix(df_copy[list(df.isnull().sum().nlargest(19).index)], sparkline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante un mapa de calor podemos identificar la correlación entre los valores faltantes, para determinar si existe algún tipo de dependencia en la ausencia de éstos datos. En efecto, notamos como las variables asociadas a las estadísticas incluidas en la sección P1.6 presentan correlación 1 entre ellas y con `upz`, confirmando entonces la clara dependencia de la ausencia de estos datos en base a la ausencia de `upz`. Además, notamos una trivial correlación de 1 entre `surface` y `price_per_m2` debido a que la data faltante(surface=0) genera una imposibilidad en el cálculo de `price_per_m2`(sería infinito)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:45.143519Z",
     "start_time": "2020-05-21T02:24:44.042465Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = [15, 10])\n",
    "msno.heatmap(df_copy, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A través de un dendograma podemos confirmar la relación de datos faltantes, ahora entre grupos. Confirmamos lo mencionado con las estadísticas y `upz`, junto con la relacion `surface` y `price_per_m2`.\n",
    "\n",
    "Se observa cómo `product_type` no tiene relación alguna en su ausencia de datos con variables como `property_type`, `surface`, sino mas bien en los rangos de valor de éstas. \n",
    "\n",
    "Además notamos que la ausencia de datos en `n_garajes` tambien es ajena a la ausencia de datos en otras variables expuestas acá.\n",
    "\n",
    "Concluimos entonces que las variables `price_per_m2` y `property_type` presentan un tipo de mecanismo de pérdida de información del tipo MAR, pues su ausencia depende de variables que podemos observar su valor:\n",
    "\n",
    "`surface` para `price_per_m2`\n",
    "\n",
    "`property_type`, `surface` para `product_type`\n",
    "\n",
    "Por otro lado, las estadísticas cumplen la hipótesis MNAR, ya que la ausencia de información en estas variables se explica por la variable ausente `upz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:45.675098Z",
     "start_time": "2020-05-21T02:24:45.145515Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "msno.dendrogram(df_copy[list(df_copy.isnull().sum().nlargest(19).index)], ax=ax,orientation='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como los NaN asociados a `upz` corresponden a una perdida de informacion del tipo MNAR, nuestro tratamiento será entonces dropear tales filas. Como los NaN asociados a `product_type` corresponden a valores que ensucian la variable respuesta, tambien se procede a dropear tales filas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:45.703051Z",
     "start_time": "2020-05-21T02:24:45.677094Z"
    }
   },
   "outputs": [],
   "source": [
    "df_copy.dropna(subset=[('categorical', 'upz'),('categorical','product_type')], axis =0,how='any', inplace=True)\n",
    "df_copy.reset_index(drop=True,inplace=True)\n",
    "df_raw = df_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:45.895121Z",
     "start_time": "2020-05-21T02:24:45.705019Z"
    }
   },
   "outputs": [],
   "source": [
    "df_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:46.045103Z",
     "start_time": "2020-05-21T02:24:45.901105Z"
    }
   },
   "outputs": [],
   "source": [
    "# Obtenemos las modas por columna\n",
    "cols= indexer(['n_rooms','n_bath','n_garajes'])\n",
    "modes = df_copy[cols].mode(axis=0,dropna=True)\n",
    "modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:46.159794Z",
     "start_time": "2020-05-21T02:24:46.048094Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generamos un diccionario de mapeos a utilizar en la imputación\n",
    "fill_dict={'n_garajes': modes[('categorical','n_garajes')][0],\n",
    "          'n_rooms': modes[('categorical','n_rooms')][0],\n",
    "          'n_bath': modes[('categorical','n_bath')][0]\n",
    "          }\n",
    "df_copy[cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:46.384195Z",
     "start_time": "2020-05-21T02:24:46.162787Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se procede a imputar\n",
    "for col in cols:\n",
    "    df_copy.loc[:,col] = df_copy.fillna(modes[col][0])\n",
    "df_copy[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:46.480938Z",
     "start_time": "2020-05-21T02:24:46.387187Z"
    }
   },
   "outputs": [],
   "source": [
    "# Notamos que todos las modas fueron imputadas\n",
    "df_copy[cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos como distribuye la variable categorica `upz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:49.811030Z",
     "start_time": "2020-05-21T02:24:46.483928Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(df_copy[('categorical','upz')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generaremos un dataframe identico al actual pero con los cambios realizados de P2.2 para comparar visualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:49.903783Z",
     "start_time": "2020-05-21T02:24:49.814021Z"
    }
   },
   "outputs": [],
   "source": [
    "df_mod = df_copy.copy()\n",
    "df_mod =  df_mod[(df_mod[('numeric','price_per_m2')]<=10**5) & (df_mod[('numeric','surface')]<=10**3)] \n",
    "df_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscamos clusterizar los `upz` en base a su valor en `price_per_m2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:50.357565Z",
     "start_time": "2020-05-21T02:24:49.908767Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se propone clusterizar la variable upz mediante kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clust = 3 # número de clusters a trabajar\n",
    "\n",
    "# Se inicializan en paralelo 2 clusterizaciones\n",
    "clusterizer = KMeans(n_clusters=n_clust)\n",
    "clusterizer_mod = KMeans(n_clusters=n_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:50.554040Z",
     "start_time": "2020-05-21T02:24:50.360560Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clustering mediante la variable de respuesta\n",
    "# Agrupamos datos por upz y tomamos promedio en la variable de respuesta\n",
    "\n",
    "# Generamos agrupaciones\n",
    "grouped_df = df_copy.groupby(by=('categorical','upz')).mean()[('numeric','price_per_m2')]\n",
    "grouped_df_mod = df_mod.groupby(by=('categorical','upz')).mean()[('numeric','price_per_m2')]\n",
    "\n",
    "# Guardamos los labels en una serie indexados por el codigo upz\n",
    "X = clusterizer.fit_predict(grouped_df.to_numpy().reshape(-1,1))\n",
    "X_mod = clusterizer_mod.fit_predict(grouped_df_mod.to_numpy().reshape(-1,1))\n",
    "\n",
    "# Generamos los labels\n",
    "labels = pd.Series(X, index=grouped_df.index, name=('categorical', 'upz_cluster'))\n",
    "labels_mod = pd.Series(X_mod, index=grouped_df_mod.index, name=('categorical', 'upz_cluster'))\n",
    "\n",
    "# Se hace merge de los labels en las datas respectivas\n",
    "df_clust = pd.merge(df_copy, labels, left_on = [('categorical', 'upz')], \n",
    "                    right_on=labels.index ,left_index=True,how='left')\n",
    "df_clust.loc[:,('categorical', 'upz_cluster')] = df_clust[('categorical', 'upz_cluster')].map(lambda x: str(int(x)))\n",
    "df_clust.reset_index(inplace = True,drop = True )\n",
    "\n",
    "df_clust_mod = pd.merge(df_mod, labels_mod, left_on = [('categorical', 'upz')], \n",
    "                    right_on=labels_mod.index ,left_index=True,how='left')\n",
    "df_clust_mod.loc[:,('categorical', 'upz_cluster')] = df_clust_mod[('categorical', 'upz_cluster')].map(lambda x: str(int(x)))\n",
    "df_clust_mod.reset_index(inplace = True,drop = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:50.569000Z",
     "start_time": "2020-05-21T02:24:50.557033Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vemos los labels de cada data \n",
    "print('Sin filtrar: ',df_clust[('categorical','upz_cluster')].unique())\n",
    "print('Filtrada: ',df_clust_mod[('categorical','upz_cluster')].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí notamos un comportamiento extraño en la distribución de los valores por categoría, nuestra hipótesis es que se debe a la potencial presencia de outliers asociados a la variable `price_per_m2`. Sin embargo al estar en escala logarítmica podemos observar una potencial diferencia en las medias por grupo, para confirmar ésto, haremos un test oneway ANOVA sobre `upz_cluster` vs `price_per_m2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:51.534446Z",
     "start_time": "2020-05-21T02:24:50.571993Z"
    }
   },
   "outputs": [],
   "source": [
    "categoricalplot(df_clust,'upz_cluster',log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:51.578337Z",
     "start_time": "2020-05-21T02:24:51.536413Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Se realiza el test oneway ANOVA para comprobar la diferencia de medias en la data sin filtrar\n",
    "'''\n",
    "idx = indexer(['price_per_m2','upz_cluster'], t_c=df_clust.columns)\n",
    "grouped = df_clust[idx].groupby(idx[1])\n",
    "total_groups = grouped.groups.keys()\n",
    "groups = [group_cleaner(grouped.get_group(str(i)), 'upz_cluster', df_clust) for i in range(n_clust)]\n",
    "\n",
    "F, p = f_oneway(*groups)\n",
    "print('Estadistico F:',F)\n",
    "print('p valor :', p)\n",
    "alfa = 0.05\n",
    "print('p<=alfa: ',p <= alfa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos qué ocurre si consideramos las modificaciones realizadas en P2.2 a éste dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:52.102025Z",
     "start_time": "2020-05-21T02:24:51.580298Z"
    }
   },
   "outputs": [],
   "source": [
    "categoricalplot(df_clust_mod,'upz_cluster',log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que la mejora es considerable en cuanto a la identificación de `price_per_m2` mediante esta clusterización, además podemos observar que la cantidad de datos que se dropean con tal de mejorar a ésta magnitud las representaciones e identificaciones solo corresponde a un 0.4% del total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:52.110971Z",
     "start_time": "2020-05-21T02:24:52.103990Z"
    }
   },
   "outputs": [],
   "source": [
    "diff = df_clust.shape[0]-df_clust_mod.shape[0]\n",
    "perc_diff = 100 *(df_clust.shape[0]-df_clust_mod.shape[0])/df_clust.shape[0]\n",
    "print('Luego del filtro como en P2.2, solo {} datos son dropeados, lo cual corresponde a un {}% de la cantidad total de datos'.format(diff,perc_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza el test oneway ANOVA con la data filtrada como en P2.2 para comprobar la diferencia de medias. Ésto junto con lo expresado en las idenrificaciónes P2.2 creemos que es evidencia suficiente para seguir trabajando con la data filtrada como en P2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:52.292992Z",
     "start_time": "2020-05-21T02:24:52.112966Z"
    }
   },
   "outputs": [],
   "source": [
    "idx = indexer(['price_per_m2','upz_cluster'], t_c=df_clust.columns)\n",
    "grouped_mod = df_clust_mod[idx].groupby(idx[1])\n",
    "total_groups_mod = grouped_mod.groups.keys()\n",
    "groups_mod = [group_cleaner(grouped_mod.get_group(str(i)), 'upz_cluster', df_clust_mod) for i in range(n_clust)]\n",
    "\n",
    "F, p = f_oneway(*groups_mod)\n",
    "print('Estadistico F:',F)\n",
    "print('p valor :', p)\n",
    "alfa = 0.05\n",
    "print('p<=alfa: ',p <= alfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:52.474690Z",
     "start_time": "2020-05-21T02:24:52.295982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Asignamos al df_raw éstos labels\n",
    "df_raw = pd.merge(df_raw, labels_mod, left_on = [('categorical', 'upz')], \n",
    "                    right_on=labels_mod.index ,left_index=True,how='left',validate='one_to_one')\n",
    "df_raw.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:52.723343Z",
     "start_time": "2020-05-21T02:24:52.477681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Actualizamos df con el clustering realizado en la parte anterior para la data filtrada\n",
    "df = df_clust_mod.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora si existe dependencia entre variables de interés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:24:52.867082Z",
     "start_time": "2020-05-21T02:24:52.726289Z"
    }
   },
   "outputs": [],
   "source": [
    "interest = ['price','surface','metrocuadrado_index','personas',\n",
    "            'indice_inseguridad','price_per_m2',\n",
    "            'n_rooms','n_garajes','n_bath']\n",
    "idxs = indexer(interest)\n",
    "idxs.sort()\n",
    "idxs.remove(('numeric', 'price_per_m2'))\n",
    "idxs.append(('numeric', 'price_per_m2'))\n",
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:06.133628Z",
     "start_time": "2020-05-21T02:24:52.872035Z"
    }
   },
   "outputs": [],
   "source": [
    "data = df.reindex(idxs, axis=1).droplevel(0,axis=1).dropna()\n",
    "sns.pairplot(data=data, diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos correlacion entre variables por medio de la matríz de correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:06.204437Z",
     "start_time": "2020-05-21T02:25:06.137619Z"
    }
   },
   "outputs": [],
   "source": [
    "corrmatrix = df.corr()\n",
    "corrmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:06.281232Z",
     "start_time": "2020-05-21T02:25:06.208427Z"
    }
   },
   "outputs": [],
   "source": [
    "col = indexer(['price_per_m2'])\n",
    "corrmatrix[col].nlargest(20,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primera instancia analizamos la correlación entre todas las variables numéricas mediante un mapa de calor(la varible objetivo están en la última fila)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:06.958422Z",
     "start_time": "2020-05-21T02:25:06.284223Z"
    }
   },
   "outputs": [],
   "source": [
    "corrmat = df['numeric'].corr()\n",
    "columnas = list(corrmat.columns)\n",
    "\n",
    "corrmat = corrmat.reindex(index = columnas, columns = columnas)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[14, 12])\n",
    "\n",
    "sns.heatmap(corrmat, vmin=-.5, vmax=.9, linewidths=.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede apreciar alta correlación (positiva y negativa) entre algunos pares de variables. Para ubicarlos hacemos un rearreglo 1D multi-índice y buscamos los que tengan módulo más alto(distinto de 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:06.979366Z",
     "start_time": "2020-05-21T02:25:06.961412Z"
    }
   },
   "outputs": [],
   "source": [
    "unoD=corrmat.stack()\n",
    "unoD[unoD[unoD<1].abs().nlargest(20).index][::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se acordó un umbral de 0.7 para definir una correlación potencialmente problemática en cuanto a la colinearidad.\n",
    "\n",
    "En vista de lo anterior notamos una estrecha relación entre los grupos\n",
    "\n",
    "`jovenes_14_24_anos_nini_perc`\n",
    "`adultos_mayores_pobres_perc`\n",
    "`indice_envegecimiento`\n",
    "\n",
    "`personas`\n",
    "`densidad_poblacion`\n",
    "\n",
    "de las cuales elegimos una de cada grupo (la última) para evitar colinearidad.\n",
    "\n",
    "Cabe destacar que el par (`price`,`surface`) también tiene un valor muy alto, pero como estas variables no se considerarán al momento de estimar `price_per_m2` al ser las generadoras de esta variable, no se estudia con mayor detalle.\n",
    "\n",
    "Por otro lado vemos que `metrocuadrado_index` tiene una alta correlación con la variable a explicar, por lo que es un buen candidato para la selección final. Para ver cómo se relacionan las otras variables con `price_per_m2` analizamos su columna.\n",
    "\n",
    "Incluyendo ahora las variables que no presentan una correlación superior al umbral, tenemos a priori los siguientes candidatos a ser incluidos en el modelo final:\n",
    "\n",
    "`densidad_poblacion` \n",
    "\n",
    "`indice_envegecimiento`\n",
    "\n",
    "`jefe_mujer_perc` \n",
    "\n",
    "`areas_verdes_perc` \n",
    "\n",
    "`trabajoinf_ninos_5_17_anos_perc`\n",
    "\n",
    "`trabajoinfampliado_ninos_5_17_anos_perc`\n",
    "\n",
    "`indice_inseguridad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:07.134817Z",
     "start_time": "2020-05-21T02:25:06.982357Z"
    }
   },
   "outputs": [],
   "source": [
    "corrmat['price_per_m2'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos las siguientes variables categóricas de interés\n",
    "\n",
    "`upz_cluster`\n",
    "\n",
    "`product_type`\n",
    "\n",
    "`n_rooms`\n",
    "\n",
    "`n_bath`\n",
    "\n",
    "`n_garajes`\n",
    "\n",
    "`furnished`\n",
    "\n",
    "Buscamos ahora verificar estadísticamente si las variables generan una diferencia significativa entre los grupos `price_per_m2`. Para ello haremos test oneway ANOVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:07.526266Z",
     "start_time": "2020-05-21T02:25:07.134817Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Recordemos que 'upz_cluster' y 'n_garajes' ya fueron validadas en P2.2 y P2.4 respectivamente\n",
    "categoric_vars = ['furnished','product_type', 'n_rooms', 'n_bath']\n",
    "\n",
    "for col in categoric_vars:\n",
    "    print('{}:'.format(col))\n",
    "    idx = indexer(['price_per_m2',col])\n",
    "    grouped = df[idx].groupby(idx[1])\n",
    "    total_groups = grouped.groups.keys()\n",
    "    groups = [group_cleaner(grouped.get_group(i), col) for i in total_groups]\n",
    "    \n",
    "    F,p = f_oneway(*groups)\n",
    "    print('Estadistico F:',F)\n",
    "    print('p valor :', p)\n",
    "    alpha = 0.05\n",
    "    reject= p <= alpha\n",
    "    if reject:\n",
    "        print('Se rechaza la nula para {}'.format(col))\n",
    "    else:\n",
    "        print('NO se rechaza la nula para {}'.format(col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que para cada variable considera se rechaza la nula, por lo tanto cada variable presenta una separación estadística considerable para `price_per_m2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T06:05:02.484933Z",
     "start_time": "2020-05-20T06:05:02.476342Z"
    }
   },
   "source": [
    "Nos interesa ahora ver si existe algun tipo de relación entre las variables numéricas y categóricas que estamos considerando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:07.870343Z",
     "start_time": "2020-05-21T02:25:07.528259Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Procedemos a testear la significancia de cada variable en la\n",
    "descripción de 'price_per_m2' utilizando el test t.\n",
    "'''\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "numeric_vars = ['densidad_poblacion', 'indice_envegecimiento', 'jefe_mujer_perc', \n",
    "               'areas_verdes_perc', 'trabajoinf_ninos_5_17_anos_perc', \n",
    "               'trabajoinfampliado_ninos_5_17_anos_perc', 'indice_inseguridad']\n",
    "\n",
    "ppm2 = np.array(df[('numeric','price_per_m2')])\n",
    "\n",
    "alfa = 0.05\n",
    "n = len(numeric_vars)\n",
    "reject_matrix = np.zeros((n,n))\n",
    "for i in range(n):\n",
    "    var1 = numeric_vars[i]\n",
    "    arr1 = np.array(df[('numeric',var1)].dropna())\n",
    "    print(var1)\n",
    "    for j in range(i+1,n):\n",
    "        var2 = numeric_vars[j]\n",
    "        arr2 = np.array(df[('numeric',var2)].dropna())\n",
    "        #t,p = testt_ind(arr1,arr2,equal_var=False) # Test T para ver la diferencia de medias\n",
    "        t,p = ks_2samp(arr1,arr2) # Test KS para ver si vienen de la misma distribución\n",
    "        print('{}:\\n Valor del estadístico: {}\\n Valor de p: {}'.format(var2,t,p))\n",
    "        reject = p < alfa\n",
    "        if reject:\n",
    "            reject_matrix[i][j] = 1\n",
    "            #print('Se rechaza la nula, {} tiene significancia'.format(var))\n",
    "            \n",
    "reject_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos un duplicado del df obtenido de la clusterización en P2.4 para generar ciertas transformaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:08.106767Z",
     "start_time": "2020-05-21T02:25:07.874333Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_transformed = df_clust.copy()\n",
    "df_transformed= df_clust_mod.copy()\n",
    "df_transformed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:08.407584Z",
     "start_time": "2020-05-21T02:25:08.109759Z"
    }
   },
   "outputs": [],
   "source": [
    "columnas = ['UPlArea','price','surface','metrocuadrado_index','personas',\n",
    "           'trabajoinf_ninos_5_17_anos_perc','trabajoinfampliado_ninos_5_17_anos_perc',\n",
    "           'jovenes_14_24_anos_nini_perc','indice_envegecimiento','jefe_mujer_perc',\n",
    "            'adultos_mayores_pobres_perc','indice_inseguridad','areas_verdes_perc',\n",
    "            'areas_verdes_perc','densidad_poblacion','price_per_m2',\n",
    "            \n",
    "            'n_garajes','upz_cluster','furnished','rent_type','n_rooms','product_type',\n",
    "            'property_type','n_bath']\n",
    "\n",
    "cols = indexer(columnas, t_c = df_transformed.columns)\n",
    "df_transformed = df_transformed[cols]\n",
    "df_transformed.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos transformar algunas variables categoricas a labels según corresponda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:08.518288Z",
     "start_time": "2020-05-21T02:25:08.410576Z"
    }
   },
   "outputs": [],
   "source": [
    "numerics= indexer(['UPlArea','price','surface','metrocuadrado_index','personas',\n",
    "           'trabajoinf_ninos_5_17_anos_perc','trabajoinfampliado_ninos_5_17_anos_perc',\n",
    "           'jovenes_14_24_anos_nini_perc','indice_envegecimiento','jefe_mujer_perc',\n",
    "           'adultos_mayores_pobres_perc','indice_inseguridad','areas_verdes_perc',\n",
    "           'areas_verdes_perc','densidad_poblacion','price_per_m2'], t_c = df_transformed.columns)\n",
    "ordinales = indexer(['n_rooms','n_bath','n_garajes'], t_c = df_transformed.columns)\n",
    "no_ordinales = indexer(['furnished','rent_type','property_type','product_type','upz_cluster'], t_c = df_transformed.columns)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:08.842498Z",
     "start_time": "2020-05-21T02:25:08.520282Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = df_transformed[numerics].dropna().copy()\n",
    "idx = data.index\n",
    "scaler=MinMaxScaler()\n",
    "newdata = scaler.fit_transform(data)\n",
    "newdata = pd.DataFrame(newdata, columns=pd.MultiIndex.from_tuples(numerics))\n",
    "df_transformed.loc[:,numerics] = newdata\n",
    "\n",
    "for col in ordinales:\n",
    "    data = df_transformed[col].dropna().copy()\n",
    "    idx = data.index\n",
    "    enc = OrdinalEncoder()\n",
    "    X = data.values.reshape([-1,1])\n",
    "    transformed = enc.fit_transform(X)\n",
    "    newcol = pd.Series(data = transformed.flatten(), index= idx)\n",
    "    df_transformed.loc[idx,col] = newcol\n",
    "    \n",
    "for col in no_ordinales:\n",
    "    data = df_transformed[col].dropna().copy()\n",
    "    idx = data.index\n",
    "    ohenc = OneHotEncoder(categories='auto',sparse=False)\n",
    "    X = data.values.reshape([-1,1])\n",
    "    transformed = ohenc.fit_transform(X)\n",
    "    feature_names = ohenc.get_feature_names()\n",
    "    feature_names = [name.replace('x0_','{}_'.format(col[1])) for name in feature_names]\n",
    "    feature_names = [('categorical',name) for name in feature_names]\n",
    "    newcols = pd.DataFrame(data = transformed, columns=pd.MultiIndex.from_tuples(feature_names), index= idx)\n",
    "    df_transformed = df_transformed.drop(columns=col).join(newcols)\n",
    "\n",
    "df_transformed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:08.950671Z",
     "start_time": "2020-05-21T02:25:08.844414Z"
    }
   },
   "outputs": [],
   "source": [
    "df_transformed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:09.135402Z",
     "start_time": "2020-05-21T02:25:08.952635Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "outlier_detection = DBSCAN(min_samples = 2, eps = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:12.993446Z",
     "start_time": "2020-05-21T02:25:09.137399Z"
    }
   },
   "outputs": [],
   "source": [
    "# se clusteriza y se obtiene la proporción de outliers\n",
    "X = df_transformed.to_numpy()\n",
    "ol = outlier_detection.fit_predict(X)\n",
    "(ol == -1).sum()/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:13.083206Z",
     "start_time": "2020-05-21T02:25:12.996439Z"
    }
   },
   "outputs": [],
   "source": [
    "# asignamos los clusters a una nueva columna y se mapea de forma que outlier=-1, inlier=1\n",
    "df_clust_mod[('categorical', 'outlier')] = ol\n",
    "data = df_clust_mod.copy()\n",
    "data.columns = data.columns.droplevel()\n",
    "data['outlier'] = data['outlier'].map(lambda x: 1 if x >=0 else -1)\n",
    "\n",
    "# se crea una tabla de doble entrada para visualizar las distribuciones\n",
    "kwargs = {'index': data['upz_cluster'], 'columns': data['outlier']}\n",
    "table = pd.crosstab(**kwargs, margins=True)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:13.224886Z",
     "start_time": "2020-05-21T02:25:13.086199Z"
    }
   },
   "outputs": [],
   "source": [
    "# Proporción de outliers por cluster upz\n",
    "table.iloc[:,0]/table['All']*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primera vista es posible decir que en proporción, los outliers no están distribuidos uniformemente a lo largo de los upz_clusters, por lo que aprovechando que las frecuencias observadas son mayores que 5 haremos un test $\\chi^2$ para asegurar esta dependencia. si las frecuencias esperadas son también mayores que 5, entonces se puede asegurar la confiabilidad del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:13.381071Z",
     "start_time": "2020-05-21T02:25:13.227850Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "chi2, p, dof, expected = chi2_contingency(table)\n",
    "print(p < 0.01)\n",
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:13.555602Z",
     "start_time": "2020-05-21T02:25:13.386056Z"
    }
   },
   "outputs": [],
   "source": [
    "# repetimos el proceso esta vez viendo la distribución de outliers con respecto a product_type\n",
    "kwargs['index'] = data['product_type']\n",
    "table = pd.crosstab(**kwargs, margins=True)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:13.630402Z",
     "start_time": "2020-05-21T02:25:13.557597Z"
    }
   },
   "outputs": [],
   "source": [
    "# Proporción de outliers por property type\n",
    "table.iloc[:,0]/table['All']*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:13.779386Z",
     "start_time": "2020-05-21T02:25:13.633395Z"
    }
   },
   "outputs": [],
   "source": [
    "chi2, p, dof, expected = chi2_contingency(table)\n",
    "print(p < 0.01)\n",
    "expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, dado lo analizado anteriormente se seleccionan las siguientes variables para realizar la estimación de `price_per_m2`:\n",
    "\n",
    "1. *Numéricas (8):* `densidad_poblacion`, `metrocuadrado_index`, `jefe_mujer_perc`, `indice_envegecimiento`, `areas_verdes_perc`, `trabajoinf_ninos_5_17_anos_perc`, `trabajoinfampliado_ninos_5_17_anos_perc` y `indice_inseguridad`\n",
    "2. *Categóricas ordinales (3):* `n_rooms`, `n_bath` y `n_garajes`\n",
    "3. *Categóricas no ordinales (3):* `product_type`, `upz_cluster` y `furnished`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:13.934637Z",
     "start_time": "2020-05-21T02:25:13.790352Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,RegressorMixin\n",
    "\n",
    "class RegresionBayesianaEmpirica(BaseEstimator,RegressorMixin):\n",
    "    \"\"\" \n",
    "    Clase derivada (herencia multiple) que implementa la heurística estudiada para\n",
    "    aproximar los hiperparámetros óptimos α y β. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.01, beta=0.01, tol=1e-5, maxiter=200):\n",
    "        '''\n",
    "        Atributos\n",
    "        ----------\n",
    "        alpha : float\n",
    "            Valor inicial para α\n",
    "        beta : float\n",
    "            Valor inicial para y β\n",
    "        tol : float\n",
    "            tolerancia objetivo. El algoritmo termina si la diferencia entre actualizaciones\n",
    "            de α y β es menor que este valor\n",
    "        maxiter : int\n",
    "            número máximo de iteraciones que realiza el algoritmo de aproximación\n",
    "        X : ndarray\n",
    "            matriz de observaciones\n",
    "        y : ndarray\n",
    "            vector con la variable objetivo asociada a las observaciones\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.tol=tol\n",
    "        self.maxiter = maxiter\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "    \n",
    "    @property\n",
    "    def X(self):\n",
    "        return self.__X\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self.__y\n",
    "    \n",
    "    @X.setter\n",
    "    def X(self, X):\n",
    "        self.__X = X\n",
    "        \n",
    "    @y.setter\n",
    "    def y(self,y):\n",
    "        self.__y = y\n",
    "\n",
    "    \n",
    "    def get_posteriori(self, X, y, alpha, beta):\n",
    "        \n",
    "        '''Calcula la probabilidad a posteriori de los parámetros\n",
    "        de la regresión lineal.\n",
    "        \n",
    "        Parámetros\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            matriz de observaciones\n",
    "        y : ndarray\n",
    "            vector con la variable objetivo asociada a las observaciones\n",
    "        alpha : float\n",
    "            recíproco de la varianza de la distribución gaussiana isotrópica\n",
    "        beta : float\n",
    "            recíproco de la varianza del error de aproximación de la regresión lineal\n",
    "\n",
    "        '''\n",
    "        S_n_inv = alpha * np.eye(X.shape[1]) + beta * X.T.dot(X)\n",
    "        S_n = np.linalg.inv(S_n_inv)\n",
    "        m_n = beta * S_n.dot(X.T).dot(y)\n",
    "        return m_n, S_n\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        ''' Realiza el esquema de aproximación estudiado para\n",
    "        optimizar los valores de α y β.\n",
    "\n",
    "        Parámetros\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            matriz de observaciones\n",
    "        y : ndarray\n",
    "            vector con la variable objetivo asociada a las observaciones\n",
    "        \n",
    "        '''\n",
    "        # Se verifican las dimensiones\n",
    "        if X.shape[0] == len(y):\n",
    "            # Se asocia al objeto la data con la que se realizó el fit\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "        else:\n",
    "            raise ValueError('la dimensión de y debe coincidir con la cantidad de filas que X')\n",
    "\n",
    "        #Se inicializan las iteraciones y el diff \n",
    "        iterations=0\n",
    "        diff = 2*self.tol\n",
    "        # Se obtienen los parámetros alfa,beta y se calcular m_n, S_n en funcion de éstos\n",
    "        alfa = self.alpha\n",
    "        beta = self.beta\n",
    "        propios=[np.real_if_close(val) for val in np.linalg.eig(beta*X.T.dot(X))[0]]\n",
    "        \n",
    "        while (diff >= self.tol) and (iterations <= self.maxiter):\n",
    "        \n",
    "            m_n, S_n = self.get_posteriori(X, y, alfa, beta)\n",
    "            gamma = np.sum([val/(alpha+val) for val in propios])\n",
    "            \n",
    "            # Se calculan los nuevos alfa y beta\n",
    "            new_alfa = gamma / (m_n.T.dot(m_n))\n",
    "            \n",
    "            new_beta = 1/(1/(X.shape[0]-gamma) * np.sum([(y[i]-m_n.T.dot(X[i,:]))**2 for i in range(X.shape[0])]))\n",
    "            \n",
    "            # Se guarda al cambio de los parámetros en norma l2\n",
    "            diff = np.max([abs(alfa-new_alfa), abs(beta-new_beta)])\n",
    "            \n",
    "            # Se muestra en pantalla el estado actual\n",
    "            if iterations%50==0:\n",
    "                print('Iteración {}:\\nalpha = {}\\nbeta = {}\\n'.format(iterations,new_alfa,new_beta))\n",
    "            \n",
    "            # Se fijan los nuevos parámetros\n",
    "            alfa = new_alfa\n",
    "            beta = new_beta\n",
    "                \n",
    "            # Se sigue iterando\n",
    "            iterations+=1\n",
    "        \n",
    "        self.alpha = alfa\n",
    "        self.beta = beta\n",
    "        print('Fit Terminado en la iteración {}, con diferencias entre actualizaciones (en norma 2) {}'.format(iterations,diff))\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X_,return_std=False):\n",
    "        '''Calcula la distribución posterior predictiva para cada nueva observación\n",
    "        y asigna la media como estimación, retornando opcionalmente la desviación.\n",
    "        \n",
    "        Parámetros\n",
    "        ----------\n",
    "        X_ : ndarray\n",
    "            matriz de nuevas observaciones\n",
    "        return_std : bool\n",
    "            indica si se retorna la desviación de la distribución predictiva\n",
    "        '''\n",
    "        m_n, S_n = self.get_posteriori(self.X,self.y,self.alpha,self.beta)\n",
    "        \n",
    "        y_ = X_.dot(m_n)\n",
    "        y_std = []\n",
    "        for i in range(len(X_)):\n",
    "            x = X_[i]\n",
    "            sigma2_n = 1/self.beta + x.T.dot(S_n).dot(x)\n",
    "            y_std.append(np.sqrt(sigma2_n))\n",
    "        \n",
    "        if return_std:\n",
    "            return y_, y_std\n",
    "        else:\n",
    "            return y_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos realizando un test de normalidad sobre las variables numéricas dado que los valóres más extremos fueron removidos por los argumentos presentados en anteriores preguntas. De esta forma se decidirá cuales de las variables numéricas con aptas para usar `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:14.121494Z",
     "start_time": "2020-05-21T02:25:13.937624Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import normaltest as nt\n",
    "print(df_raw['categorical'].columns)\n",
    "s, p = nt(df_raw['numeric'], axis=0)\n",
    "p<0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una copia de la data a trabajar, dropeando las variables miscelaneas junto con `price`, `surface`, `property_type` y `upz`. Además dropeamos el nivel más externo del multiíndice por comodidad. Finalmente se dropean los valores extremos encontrados en P2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:14.245409Z",
     "start_time": "2020-05-21T02:25:14.123488Z"
    }
   },
   "outputs": [],
   "source": [
    "d = df_raw.drop(columns='miscelaneous', level=0).drop(columns=['price', 'surface', 'property_type', 'upz'],level=1).copy()\n",
    "d.columns = d.columns.droplevel()\n",
    "d = d[(d['price_per_m2']>1)&(d['price_per_m2']<10**5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:14.362693Z",
     "start_time": "2020-05-21T02:25:14.245409Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se hacen los imports necesarios para las transformaciones\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos ahora listas de variables acorde al tratamiento a darles. Dados los resultados del test de normalidad, todas las variables serán consideradas NO provenientes de una normal. Es por esta razón que no se hará uso del objeto `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:14.483370Z",
     "start_time": "2020-05-21T02:25:14.365684Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_vars = ['product_type','rent_type', 'upz_cluster', 'furnished']\n",
    "\n",
    "ordinal_vars = ['n_rooms', 'n_bath', 'n_garajes']\n",
    "ordinal_categories = [['1', '2', '3', '4', '5', '5+'],\n",
    "                      ['1', '2', '3', '4', '5', '5+'],\n",
    "                      ['1', '2', '3', '4', '4+']\n",
    "                     ]\n",
    "numeric_vars = ['UPlArea', 'metrocuadrado_index', 'personas',\n",
    "       'trabajoinf_ninos_5_17_anos_perc',\n",
    "       'trabajoinfampliado_ninos_5_17_anos_perc',\n",
    "       'jovenes_14_24_anos_nini_perc', 'indice_envegecimiento',\n",
    "       'jefe_mujer_perc', 'adultos_mayores_pobres_perc', 'indice_inseguridad',\n",
    "       'areas_verdes_perc', 'densidad_poblacion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:14.636956Z",
     "start_time": "2020-05-21T02:25:14.487359Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se generan los pipelines para los distintos tipos de variables\n",
    "\n",
    "# pipeline variables categóricas\n",
    "pipe_categorico = Pipeline(steps=[('uno_caliente_codificador', OneHotEncoder(sparse=False))])\n",
    "\n",
    "# pipeline variables ordinales\n",
    "pipe_ordinal = Pipeline(steps=[('imputador_ordinal', SimpleImputer(strategy='most_frequent')),\n",
    "                              ('ordinal_codificador', OrdinalEncoder(categories=ordinal_categories))])\n",
    "\n",
    "# pipeline numéricas (todas NO provenientes de una normal)\n",
    "pipe_numerico = Pipeline(steps=[('min_max_escalador', MinMaxScaler()),\n",
    "                               ('poly_features', PolynomialFeatures(degree=3))])\n",
    "\n",
    "# pipeline final\n",
    "pipe_shishigang = ColumnTransformer(transformers=[\n",
    "    ('pipe_categotico', pipe_categorico, categorical_vars),\n",
    "    ('pipe_ordinal', pipe_ordinal, ordinal_vars),\n",
    "    ('pipe_numerico', pipe_numerico, numeric_vars)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:14.950119Z",
     "start_time": "2020-05-21T02:25:14.639950Z"
    }
   },
   "outputs": [],
   "source": [
    "X = d.drop(columns='price_per_m2').copy()\n",
    "X_t = pipe_shishigang.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:14.961091Z",
     "start_time": "2020-05-21T02:25:14.952115Z"
    }
   },
   "outputs": [],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T03:04:23.899913Z",
     "start_time": "2020-05-20T03:04:23.892897Z"
    }
   },
   "source": [
    "## Parte 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:15.100182Z",
     "start_time": "2020-05-21T02:25:14.964083Z"
    }
   },
   "outputs": [],
   "source": [
    "X = d.drop(columns='price_per_m2').copy()\n",
    "y = d.price_per_m2.values.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:15.259390Z",
     "start_time": "2020-05-21T02:25:15.100182Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:15.383560Z",
     "start_time": "2020-05-21T02:25:15.262883Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('transformador', pipe_shishigang),\n",
    "                       ('regresor', RegresionBayesianaEmpirica(alpha=1e-5, beta=1e-5, tol=1e-9))]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:16.417278Z",
     "start_time": "2020-05-21T02:25:15.385555Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:16.427251Z",
     "start_time": "2020-05-21T02:25:16.420270Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe['regresor'].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:17.746587Z",
     "start_time": "2020-05-21T02:25:16.430243Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred, y_std = pipe.predict(X_test,return_std=True)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rms = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:17.756560Z",
     "start_time": "2020-05-21T02:25:17.749579Z"
    }
   },
   "outputs": [],
   "source": [
    "np.std(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:18.194588Z",
     "start_time": "2020-05-21T02:25:17.758555Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:48:15.385919Z",
     "start_time": "2020-05-20T02:48:15.375946Z"
    }
   },
   "source": [
    "## Parte 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos ahora listas de variables acorde al tratamiento a darles. Dados los resultados del test de normalidad, todas las variables serán consideradas NO provenientes de una normal. Es por esta razón que no se hará uso del objeto `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:18.670902Z",
     "start_time": "2020-05-21T02:25:18.194588Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_vars = ['product_type', 'upz_cluster', 'furnished']\n",
    "\n",
    "ordinal_vars = ['n_rooms', 'n_bath', 'n_garajes']\n",
    "ordinal_categories = [['1', '2', '3', '4', '5', '5+'],\n",
    "                      ['1', '2', '3', '4', '5', '5+'],\n",
    "                      ['1', '2', '3', '4', '4+']\n",
    "                     ]\n",
    "numeric_vars = ['metrocuadrado_index',\n",
    "       'trabajoinf_ninos_5_17_anos_perc',\n",
    "       'trabajoinfampliado_ninos_5_17_anos_perc', 'indice_envegecimiento',\n",
    "       'jefe_mujer_perc', 'indice_inseguridad',\n",
    "       'areas_verdes_perc', 'densidad_poblacion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:18.937590Z",
     "start_time": "2020-05-21T02:25:18.672897Z"
    }
   },
   "outputs": [],
   "source": [
    "# Se generan los pipelines para los distintos tipos de variables\n",
    "\n",
    "# pipeline variables categóricas\n",
    "pipe_categorico = Pipeline(steps=[('uno_caliente_codificador', OneHotEncoder(sparse=False, categories='auto'))])\n",
    "\n",
    "# pipeline variables ordinales\n",
    "pipe_ordinal = Pipeline(steps=[('imputador_ordinal', SimpleImputer(strategy='most_frequent')),\n",
    "                              ('ordinal_codificador', OrdinalEncoder(categories=ordinal_categories))])\n",
    "\n",
    "# pipeline numéricas (todas NO provenientes de una normal)\n",
    "pipe_numerico = Pipeline(steps=[('min_max_escalador', MinMaxScaler()),\n",
    "                               ('poly_features', PolynomialFeatures(degree=3))])\n",
    "\n",
    "# pipeline transformaciones por columna\n",
    "pipe_shishigang = ColumnTransformer(transformers=[\n",
    "    ('pipe_categotico', pipe_categorico, categorical_vars),\n",
    "    ('pipe_ordinal', pipe_ordinal, ordinal_vars),\n",
    "    ('pipe_numerico', pipe_numerico, numeric_vars)\n",
    "])\n",
    "\n",
    "# pipeline final variables de interés\n",
    "pipe_shishigang_v2_4k = Pipeline(steps=[('transformador', pipe_shishigang),\n",
    "                       ('regresor', RegresionBayesianaEmpirica(alpha=1e-5, beta=1e-5, tol=1e-9))]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:19.747578Z",
     "start_time": "2020-05-21T02:25:18.939431Z"
    }
   },
   "outputs": [],
   "source": [
    "# nos quedamos con las variables de interés\n",
    "d_v2 = d[categorical_vars + ordinal_vars + numeric_vars + ['price_per_m2']].copy()\n",
    "\n",
    "# definimos X e y acorde a la nueva data\n",
    "X = d_v2.drop(columns='price_per_m2').copy()\n",
    "y = d_v2.price_per_m2.values.copy()\n",
    "\n",
    "# realizamos separación aleatoria en train set y test set con 20% para test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# fiteamos al pipe\n",
    "pipe_shishigang_v2_4k.fit(X_train, y_train)\n",
    "\n",
    "# calculamos el score sobre el test set\n",
    "pipe_shishigang_v2_4k.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:19.776499Z",
     "start_time": "2020-05-21T02:25:19.750570Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Guardamos el último modelo de regresión entrenado en un archivo pickle\n",
    "import pickle\n",
    "with open('modelo.pkl','bw') as handler:\n",
    "    pickle.dump(pipe_shishigang_v2_4k['regresor'], handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:19.888200Z",
     "start_time": "2020-05-21T02:25:19.779493Z"
    }
   },
   "outputs": [],
   "source": [
    "# ''' Celda que prueba 'modelo.pkl'. '''\n",
    "# ct=pipe_shishigang_v2_4k['transformador']\n",
    "# del(pipe_shishigang_v2_4k)\n",
    "# with open('modelo.pkl','br') as data:\n",
    "#     regresor = pickle.load(data)\n",
    "#     #regresor.predict(ct.fit_transform(X_train))\n",
    "#     print(regresor.score(ct.fit_transform(X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T02:25:21.105339Z",
     "start_time": "2020-05-21T02:25:19.890196Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# creamos pipeline usando el mismo transformer pero con BayesianRidge como regresor\n",
    "pipe_skgang = Pipeline(steps=[('transformador', pipe_shishigang),\n",
    "                       ('regresor', BayesianRidge(tol=1e-9))]);\n",
    "\n",
    "# fiteamos usando la misma data de la parte anterior\n",
    "pipe_skgang.fit(X_train, y_train)\n",
    "\n",
    "# calculamos el score sobre el test set\n",
    "pipe_skgang.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 422,
   "position": {
    "height": "444px",
    "left": "880px",
    "right": "20px",
    "top": "15px",
    "width": "307px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
